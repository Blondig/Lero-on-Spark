package org.apache.spark.metrics;
public  class NewCombineTextRecordReaderWrapper extends org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text> {
  public   NewCombineTextRecordReaderWrapper (org.apache.hadoop.mapreduce.lib.input.CombineFileSplit split, org.apache.hadoop.mapreduce.TaskAttemptContext context, java.lang.Integer idx)  { throw new RuntimeException(); }
  public  void close ()  { throw new RuntimeException(); }
  public  org.apache.hadoop.mapreduce.RecordReader<org.apache.hadoop.io.LongWritable, org.apache.hadoop.io.Text> delegate ()  { throw new RuntimeException(); }
  public  org.apache.hadoop.mapreduce.lib.input.FileSplit fileSplit ()  { throw new RuntimeException(); }
  public  org.apache.hadoop.io.LongWritable getCurrentKey ()  { throw new RuntimeException(); }
  public  org.apache.hadoop.io.Text getCurrentValue ()  { throw new RuntimeException(); }
  public  float getProgress ()  { throw new RuntimeException(); }
  public  void initialize (org.apache.hadoop.mapreduce.InputSplit split, org.apache.hadoop.mapreduce.TaskAttemptContext context)  { throw new RuntimeException(); }
  public  boolean nextKeyValue ()  { throw new RuntimeException(); }
}
