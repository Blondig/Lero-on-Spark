[0m[[0m[0mdebug[0m] [0m[0m[zinc] IncrementalCompile -----------[0m
[0m[[0m[0mdebug[0m] [0m[0mIncrementalCompile.incrementalCompile[0m
[0m[[0m[0mdebug[0m] [0m[0mprevious = Stamps for: 115 products, 32 sources, 8 libraries[0m
[0m[[0m[0mdebug[0m] [0m[0mcurrent source = Set(${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/producer/CachedKafkaProducer.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaWriter.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceProvider.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceInitialOffsetWriter.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaStreamingWrite.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/package.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaContinuousStream.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaBatch.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaWrite.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/package-info.java, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaBatchPartitionReader.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/JsonUtils.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaRecordToRowConverter.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/consumer/FetchedDataPool.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/consumer/KafkaDataConsumer.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/producer/InternalKafkaProducerPool.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaWriteTask.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaMicroBatchStream.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReader.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReaderAdmin.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaBatchWrite.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSource.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceOffset.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSink.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/ConsumerStrategy.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetReaderConsumer.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeCalculator.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaOffsetRangeLimit.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/consumer/InternalKafkaConsumerPool.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaRelation.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaDataWriter.scala, ${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceRDD.scala)[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating '${CSR_CACHE}/https/repo1.maven.org/maven2/org/json4s/json4s-jackson_2.12/3.7.0-M11/json4s-jackson_2.12-3.7.0-M11.jar' because org.json4s.jackson.JacksonSerialization is now provided by ${CSR_CACHE}/https/maven-central.storage-download.googleapis.com/maven2/org/json4s/json4s-jackson_2.12/3.7.0-M11/json4s-jackson_2.12-3.7.0-M11.jar[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating '${CSR_CACHE}/https/repo1.maven.org/maven2/org/json4s/json4s-core_2.12/3.7.0-M11/json4s-core_2.12-3.7.0-M11.jar' because org.json4s.NoTypeHints$ is now provided by ${CSR_CACHE}/https/maven-central.storage-download.googleapis.com/maven2/org/json4s/json4s-core_2.12/3.7.0-M11/json4s-core_2.12-3.7.0-M11.jar[0m
[0m[[0m[0mdebug[0m] [0m[0m> initialChanges = InitialChanges(Changes(added = Set(), removed = Set(), changed = Set(), unmodified = ...),Set(),Set(${CSR_CACHE}/https/repo1.maven.org/maven2/org/json4s/json4s-jackson_2.12/3.7.0-M11/json4s-jackson_2.12-3.7.0-M11.jar, ${CSR_CACHE}/https/repo1.maven.org/maven2/org/json4s/json4s-core_2.12/3.7.0-M11/json4s-core_2.12-3.7.0-M11.jar),API Changes: Set(NamesChange(org.apache.spark.sql.streaming.OutputMode,ModifiedNames(changes = UsedName(wait,[Default]), UsedName(org;apache;spark;sql;streaming;OutputMode;init;,[Default]), UsedName(Update,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(Append,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(OutputMode,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]), UsedName(Complete,[Default]))), NamesChange(org.apache.spark.sql.connector.write.Write,ModifiedNames(changes = UsedName(toBatch,[Default]), UsedName(toStreaming,[Default]), UsedName(Write,[Default]), UsedName(supportedCustomMetrics,[Default]), UsedName(description,[Default]))), NamesChange(org.apache.spark.sql.connector.catalog.SupportsRead,ModifiedNames(changes = UsedName(properties,[Default]), UsedName(newScanBuilder,[Default]), UsedName(partitioning,[Default]), UsedName(capabilities,[Default]), UsedName(schema,[Default]), UsedName(SupportsRead,[Default]), UsedName(name,[Default]))), NamesChange(org.apache.spark.sql.connector.read.Scan,ModifiedNames(changes = UsedName(readSchema,[Default]), UsedName(toBatch,[Default]), UsedName(Scan,[Default]), UsedName(toContinuousStream,[Default]), UsedName(supportedCustomMetrics,[Default]), UsedName(description,[Default]), UsedName(toMicroBatchStream,[Default]))), NamesChange(org.apache.spark.sql.connector.write.WriteBuilder,ModifiedNames(changes = UsedName(WriteBuilder,[Default]), UsedName(buildForStreaming,[Default]), UsedName(build,[Default]), UsedName(buildForBatch,[Default]))), NamesChange(org.apache.spark.sql.connector.read.PartitionReader,ModifiedNames(changes = UsedName(currentMetricsValues,[Default]), UsedName(close,[Default]), UsedName(PartitionReader,[Default]), UsedName(get,[Default]), UsedName(next,[Default]))), NamesChange(org.apache.spark.sql.connector.read.PartitionReaderFactory,ModifiedNames(changes = UsedName(supportColumnarReads,[Default]), UsedName(PartitionReaderFactory,[Default]), UsedName(createReader,[Default]), UsedName(createColumnarReader,[Default]))), NamesChange(org.apache.spark.sql.catalyst.expressions.UnsafeRow,ModifiedNames(changes = UsedName(setShort,[Default]), UsedName(setTotalSize,[Default]), UsedName(getBoolean,[Default]), UsedName(wait,[Default]), UsedName(setNotNullAt,[Default]), UsedName(writeExternal,[Default]), UsedName(getInterval,[Default]), UsedName(getDecimal,[Default]), UsedName(empty,[Default]), UsedName(getWriter,[Default]), UsedName(getSizeInBytes,[Default]), UsedName(apply,[Default]), UsedName(numFields,[Default]), UsedName(getInt,[Default]), UsedName(writeToMemory,[Default]), UsedName(write,[Default]), UsedName(setFloat,[Default]), UsedName(setBoolean,[Default]), UsedName(copyFrom,[Default]), UsedName(getByte,[Default]), UsedName(copy,[Default]), UsedName(setNullAt,[Default]), UsedName(mutableFieldTypes,[Default]), UsedName(notify,[Default]), UsedName(UnsafeRow,[Default]), UsedName(equals,[Default]), UsedName(isFixedLength,[Default]), UsedName(getShort,[Default]), UsedName(setDecimal,[Default]), UsedName(setDouble,[Default]), UsedName(getAccessor,[Default]), UsedName(org;apache;spark;sql;catalyst;expressions;UnsafeRow;init;,[Default]), UsedName(getLong,[Default]), UsedName(getMap,[Default]), UsedName(WORD_SIZE,[Default]), UsedName(copyValue,[Default]), UsedName(getUTF8String,[Default]), UsedName(createFromByteArray,[Default]), UsedName(setByte,[Default]), UsedName(read,[Default]), UsedName(getBaseObject,[Default]), UsedName(getDouble,[Default]), UsedName(getArray,[Default]), UsedName(getString,[Default]), UsedName(hashCode,[Default]), UsedName(getBinary,[Default]), UsedName(calculateBitSetWidthInBytes,[Default]), UsedName(getFloat,[Default]), UsedName(toSeq,[Default]), UsedName(getAccessor$default$2,[Default]), UsedName(getClass,[Default]), UsedName(isMutable,[Default]), UsedName(update,[Default]), UsedName(writeToStream,[Default]), UsedName($assertionsDisabled,[Default]), UsedName(anyNull,[Default]), UsedName(setLong,[Default]), UsedName(getBytes,[Default]), UsedName(pointTo,[Default]), UsedName(notifyAll,[Default]), UsedName(writeTo,[Default]), UsedName(setInt,[Default]), UsedName(fromSeq,[Default]), UsedName(getBaseOffset,[Default]), UsedName(get,[Default]), UsedName($anonfun$toSeq$1,[Default]), UsedName(getStruct,[Default]), UsedName(toString,[Default]), UsedName(setInterval,[Default]), UsedName(isNullAt,[Default]), UsedName(readExternal,[Default]), UsedName(writeFieldTo,[Default]))), NamesChange(org.apache.spark.sql.connector.write.streaming.StreamingDataWriterFactory,ModifiedNames(changes = UsedName(createWriter,[Default]), UsedName(StreamingDataWriterFactory,[Default]))), NamesChange(org.apache.spark.sql.connector.write.streaming.StreamingWrite,ModifiedNames(changes = UsedName(commit,[Default]), UsedName(createStreamingWriterFactory,[Default]), UsedName(abort,[Default]), UsedName(StreamingWrite,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ReadMinRows,ModifiedNames(changes = UsedName(org;apache;spark;sql;connector;read;streaming;ReadMinRows;init;,[Default]), UsedName(wait,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(maxTriggerDelayMs,[Default]), UsedName(minRows,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]), UsedName(ReadMinRows,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ContinuousStream,ModifiedNames(changes = UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(createContinuousReaderFactory,[Default]), UsedName(mergeOffsets,[Default]), UsedName(initialOffset,[Default]), UsedName(planInputPartitions,[Default]), UsedName(needsReconfiguration,[Default]), UsedName(ContinuousStream,[Default]), UsedName(deserializeOffset,[Default]))), NamesChange(org.apache.spark.sql.connector.metric.CustomMetric,ModifiedNames(changes = UsedName(initialValue,[Default]), UsedName(CustomMetric,[Default]), UsedName(aggregateTaskMetrics,[Default]), UsedName(description,[Default]), UsedName(name,[Default]))), NamesChange(org.apache.spark.sql.connector.catalog.Table,ModifiedNames(changes = UsedName(properties,[Default]), UsedName(Table,[Default]), UsedName(partitioning,[Default]), UsedName(capabilities,[Default]), UsedName(schema,[Default]), UsedName(name,[Default]))), NamesChange(org.apache.spark.sql.util.CaseInsensitiveStringMap,ModifiedNames(changes = UsedName(org;apache;spark;sql;util;CaseInsensitiveStringMap;init;,[Default]), UsedName(entrySet,[Default]), UsedName(containsValue,[Default]), UsedName(getBoolean,[Default]), UsedName(replaceAll,[Default]), UsedName(wait,[Default]), UsedName(forEach,[Default]), UsedName(asCaseSensitiveMap,[Default]), UsedName(replace,[Default]), UsedName(empty,[Default]), UsedName(computeIfPresent,[Default]), UsedName(getInt,[Default]), UsedName(getOrDefault,[Default]), UsedName(containsKey,[Default]), UsedName(computeIfAbsent,[Default]), UsedName(putAll,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(remove,[Default]), UsedName(put,[Default]), UsedName(getLong,[Default]), UsedName(values,[Default]), UsedName(size,[Default]), UsedName(getDouble,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(putIfAbsent,[Default]), UsedName(compute,[Default]), UsedName(notifyAll,[Default]), UsedName(CaseInsensitiveStringMap,[Default]), UsedName(isEmpty,[Default]), UsedName(merge,[Default]), UsedName(get,[Default]), UsedName(clear,[Default]), UsedName(toString,[Default]), UsedName(keySet,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ReadMaxRows,ModifiedNames(changes = UsedName(wait,[Default]), UsedName(ReadMaxRows,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(maxRows,[Default]), UsedName(notifyAll,[Default]), UsedName(org;apache;spark;sql;connector;read;streaming;ReadMaxRows;init;,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ReadAllAvailable,ModifiedNames(changes = UsedName(wait,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(ReadAllAvailable,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(INSTANCE,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.write.DataWriter,ModifiedNames(changes = UsedName(commit,[Default]), UsedName(currentMetricsValues,[Default]), UsedName(write,[Default]), UsedName(close,[Default]), UsedName(DataWriter,[Default]), UsedName(abort,[Default]))), NamesChange(org.apache.spark.sql.internal.SQLConf,ModifiedNames(changes = UsedName(sparkCostRecordEnabled,[Default]), UsedName(LAST_LEVEL_INDEX,[Default]), UsedName(tableNumLow,[Default]), UsedName(tableNumUp,[Default]), UsedName(LAST_LEVEL_INDEX_ENABLED,[Default]), UsedName(PLAN_GROUP_ENABLED,[Default]), UsedName(LAST_LEVEL_EPSILON_ENABLED,[Default]), UsedName(lastLevelIndexEnabled,[Default]), UsedName(lastLevelSize,[Default]), UsedName(lastLevelEpsilonEnabled,[Default]), UsedName(SPARK_COST_SIZE,[Default]), UsedName(LAST_LEVEL_EPSILON,[Default]), UsedName(lastLevelEpsilon,[Default]), UsedName(maxLastLevel,[Default]), UsedName(TABLE_NUM_UP,[Default]), UsedName(PLAN_GROUP_SIZE,[Default]), UsedName(sparkCostCard,[Default]), UsedName(planGroupEnabled,[Default]), UsedName(lastLevelSizeEnabled,[Default]), UsedName(TABLE_NUM_LOW,[Default]), UsedName(LAST_LEVEL_SIZE,[Default]), UsedName(LAST_LEVEL_SIZE_ENABLED,[Default]), UsedName(maxLastLevelEnabled,[Default]), UsedName(MAX_LAST_LEVEL_ENABLED,[Default]), UsedName(sparkCostSize,[Default]), UsedName(planGroupSize,[Default]), UsedName(SPARK_COST_CARD,[Default]), UsedName(SPARK_COST_RECORD_ENABLED,[Default]), UsedName(MAX_LAST_LEVEL,[Default]), UsedName(lastLevelIndex,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.CompositeReadLimit,ModifiedNames(changes = UsedName(getReadLimits,[Default]), UsedName(wait,[Default]), UsedName(org;apache;spark;sql;connector;read;streaming;CompositeReadLimit;init;,[Default]), UsedName(CompositeReadLimit,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.read.Batch,ModifiedNames(changes = UsedName(planInputPartitions,[Default]), UsedName(createReaderFactory,[Default]), UsedName(Batch,[Default]))), NamesChange(org.apache.spark.sql.connector.catalog.SupportsWrite,ModifiedNames(changes = UsedName(newWriteBuilder,[Default]), UsedName(properties,[Default]), UsedName(partitioning,[Default]), UsedName(capabilities,[Default]), UsedName(SupportsWrite,[Default]), UsedName(schema,[Default]), UsedName(name,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.PartitionOffset,ModifiedNames(changes = UsedName(PartitionOffset,[Default]))), NamesChange(org.apache.spark.sql.connector.metric.CustomTaskMetric,ModifiedNames(changes = UsedName(name,[Default]), UsedName(CustomTaskMetric,[Default]), UsedName(value,[Default]))), NamesChange(org.apache.spark.sql.connector.write.WriterCommitMessage,ModifiedNames(changes = UsedName(WriterCommitMessage,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.MicroBatchStream,ModifiedNames(changes = UsedName(latestOffset,[Default]), UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(MicroBatchStream,[Default]), UsedName(initialOffset,[Default]), UsedName(planInputPartitions,[Default]), UsedName(deserializeOffset,[Default]), UsedName(createReaderFactory,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.Offset,ModifiedNames(changes = UsedName(json,[Default]), UsedName(wait,[Default]), UsedName(Offset,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(org;apache;spark;sql;connector;read;streaming;Offset;init;,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.write.SupportsTruncate,ModifiedNames(changes = UsedName(truncate,[Default]), UsedName(buildForStreaming,[Default]), UsedName(SupportsTruncate,[Default]), UsedName(build,[Default]), UsedName(buildForBatch,[Default]))), NamesChange(org.apache.spark.sql.internal.SQLConf$,ModifiedNames(changes = UsedName(sparkCostRecordEnabled,[Default]), UsedName(LAST_LEVEL_INDEX,[Default]), UsedName(tableNumLow,[Default]), UsedName(tableNumUp,[Default]), UsedName(LAST_LEVEL_INDEX_ENABLED,[Default]), UsedName(PLAN_GROUP_ENABLED,[Default]), UsedName(LAST_LEVEL_EPSILON_ENABLED,[Default]), UsedName(lastLevelIndexEnabled,[Default]), UsedName(lastLevelSize,[Default]), UsedName(lastLevelEpsilonEnabled,[Default]), UsedName(SPARK_COST_SIZE,[Default]), UsedName(LAST_LEVEL_EPSILON,[Default]), UsedName(lastLevelEpsilon,[Default]), UsedName(maxLastLevel,[Default]), UsedName(TABLE_NUM_UP,[Default]), UsedName(PLAN_GROUP_SIZE,[Default]), UsedName(sparkCostCard,[Default]), UsedName(planGroupEnabled,[Default]), UsedName(lastLevelSizeEnabled,[Default]), UsedName(TABLE_NUM_LOW,[Default]), UsedName(LAST_LEVEL_SIZE,[Default]), UsedName(LAST_LEVEL_SIZE_ENABLED,[Default]), UsedName(maxLastLevelEnabled,[Default]), UsedName(MAX_LAST_LEVEL_ENABLED,[Default]), UsedName(sparkCostSize,[Default]), UsedName(planGroupSize,[Default]), UsedName(SPARK_COST_CARD,[Default]), UsedName(SPARK_COST_RECORD_ENABLED,[Default]), UsedName(MAX_LAST_LEVEL,[Default]), UsedName(lastLevelIndex,[Default]))), NamesChange(org.apache.spark.sql.connector.write.PhysicalWriteInfo,ModifiedNames(changes = UsedName(numPartitions,[Default]), UsedName(PhysicalWriteInfo,[Default]))), NamesChange(org.apache.spark.sql.connector.read.ScanBuilder,ModifiedNames(changes = UsedName(ScanBuilder,[Default]), UsedName(build,[Default]))), NamesChange(org.apache.spark.sql.connector.catalog.TableCapability,ModifiedNames(changes = UsedName(compareTo,[Default]), UsedName(wait,[Default]), UsedName(OVERWRITE_BY_FILTER,[Default]), UsedName(V1_BATCH_WRITE,[Default]), UsedName(MICRO_BATCH_READ,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(TableCapability,[Default]), UsedName(ordinal,[Default]), UsedName(CONTINUOUS_READ,[Default]), UsedName(STREAMING_WRITE,[Default]), UsedName(values,[Default]), UsedName(ACCEPT_ANY_SCHEMA,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(BATCH_WRITE,[Default]), UsedName(BATCH_READ,[Default]), UsedName(OVERWRITE_DYNAMIC,[Default]), UsedName(notifyAll,[Default]), UsedName(name,[Default]), UsedName(getDeclaringClass,[Default]), UsedName(TRUNCATE,[Default]), UsedName(valueOf,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.write.BatchWrite,ModifiedNames(changes = UsedName(commit,[Default]), UsedName(BatchWrite,[Default]), UsedName(onDataWriterCommit,[Default]), UsedName(createBatchWriterFactory,[Default]), UsedName(useCommitCoordinator,[Default]), UsedName(abort,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReaderFactory,ModifiedNames(changes = UsedName(supportColumnarReads,[Default]), UsedName(ContinuousPartitionReaderFactory,[Default]), UsedName(createReader,[Default]), UsedName(createColumnarReader,[Default]))), NamesChange(org.apache.spark.sql.connector.read.InputPartition,ModifiedNames(changes = UsedName(InputPartition,[Default]), UsedName(preferredLocations,[Default]))), NamesChange(org.apache.spark.sql.connector.write.DataWriterFactory,ModifiedNames(changes = UsedName(createWriter,[Default]), UsedName(DataWriterFactory,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ReportsSourceMetrics,ModifiedNames(changes = UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(ReportsSourceMetrics,[Default]), UsedName(initialOffset,[Default]), UsedName(deserializeOffset,[Default]), UsedName(metrics,[Default]))), NamesChange(org.apache.spark.sql.execution.streaming.Offset,ModifiedNames(changes = UsedName(json,[Default]), UsedName(wait,[Default]), UsedName(Offset,[Default]), UsedName(org;apache;spark;sql;execution;streaming;Offset;init;,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.SupportsTriggerAvailableNow,ModifiedNames(changes = UsedName(latestOffset,[Default]), UsedName(reportLatestOffset,[Default]), UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(getDefaultReadLimit,[Default]), UsedName(initialOffset,[Default]), UsedName(prepareForTriggerAvailableNow,[Default]), UsedName(SupportsTriggerAvailableNow,[Default]), UsedName(deserializeOffset,[Default]))), NamesChange(org.apache.spark.sql.SaveMode,ModifiedNames(changes = UsedName(compareTo,[Default]), UsedName(wait,[Default]), UsedName(ErrorIfExists,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(ordinal,[Default]), UsedName(Overwrite,[Default]), UsedName(values,[Default]), UsedName(Append,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(name,[Default]), UsedName(SaveMode,[Default]), UsedName(getDeclaringClass,[Default]), UsedName(valueOf,[Default]), UsedName(toString,[Default]), UsedName(Ignore,[Default]))), NamesChange(org.apache.spark.sql.connector.metric.CustomSumMetric,ModifiedNames(changes = UsedName(wait,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(initialValue,[Default]), UsedName(org;apache;spark;sql;connector;metric;CustomSumMetric;init;,[Default]), UsedName(CustomSumMetric,[Default]), UsedName(hashCode,[Default]), UsedName(aggregateTaskMetrics,[Default]), UsedName(getClass,[Default]), UsedName(description,[Default]), UsedName(notifyAll,[Default]), UsedName(name,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.write.LogicalWriteInfo,ModifiedNames(changes = UsedName(schema,[Default]), UsedName(options,[Default]), UsedName(LogicalWriteInfo,[Default]), UsedName(queryId,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader,ModifiedNames(changes = UsedName(currentMetricsValues,[Default]), UsedName(close,[Default]), UsedName(ContinuousPartitionReader,[Default]), UsedName(getOffset,[Default]), UsedName(get,[Default]), UsedName(next,[Default]))), NamesChange(org.apache.spark.sql.catalyst.expressions.UnsafeArrayData,ModifiedNames(changes = UsedName(calculateHeaderPortionInBytes,[Default]), UsedName(toObjectArray,[Default]), UsedName(setShort,[Default]), UsedName(UnsafeArrayData,[Default]), UsedName(getBoolean,[Default]), UsedName(wait,[Default]), UsedName(numElements,[Default]), UsedName(toIntArray,[Default]), UsedName(writeExternal,[Default]), UsedName(getInterval,[Default]), UsedName(getDecimal,[Default]), UsedName(getSizeInBytes,[Default]), UsedName(createFreshArray,[Default]), UsedName(getInt,[Default]), UsedName(writeToMemory,[Default]), UsedName(write,[Default]), UsedName(setFloat,[Default]), UsedName(setBoolean,[Default]), UsedName(toByteArray,[Default]), UsedName(getByte,[Default]), UsedName(allocateArrayData,[Default]), UsedName(copy,[Default]), UsedName(setNullAt,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(toLongArray,[Default]), UsedName(toDoubleArray,[Default]), UsedName(getShort,[Default]), UsedName(toBooleanArray,[Default]), UsedName(setDouble,[Default]), UsedName(getLong,[Default]), UsedName(getMap,[Default]), UsedName(getUTF8String,[Default]), UsedName(setByte,[Default]), UsedName(read,[Default]), UsedName(toShortArray,[Default]), UsedName(toFloatArray,[Default]), UsedName(foreach,[Default]), UsedName(getBaseObject,[Default]), UsedName(getDouble,[Default]), UsedName(getArray,[Default]), UsedName(hashCode,[Default]), UsedName(getBinary,[Default]), UsedName(toArrayData,[Default]), UsedName(getFloat,[Default]), UsedName(toSeq,[Default]), UsedName(getClass,[Default]), UsedName(update,[Default]), UsedName($assertionsDisabled,[Default]), UsedName(setLong,[Default]), UsedName(fromPrimitiveArray,[Default]), UsedName(pointTo,[Default]), UsedName(notifyAll,[Default]), UsedName(writeTo,[Default]), UsedName(setInt,[Default]), UsedName(org;apache;spark;sql;catalyst;expressions;UnsafeArrayData;init;,[Default]), UsedName(getBaseOffset,[Default]), UsedName(get,[Default]), UsedName(getStruct,[Default]), UsedName(toString,[Default]), UsedName(shouldUseGenericArrayData,[Default]), UsedName(toArray,[Default]), UsedName(isNullAt,[Default]), UsedName(array,[Default]), UsedName(readExternal,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ReadLimit,ModifiedNames(changes = UsedName(maxFiles,[Default]), UsedName(allAvailable,[Default]), UsedName(compositeLimit,[Default]), UsedName(maxRows,[Default]), UsedName(minRows,[Default]), UsedName(ReadLimit,[Default])))))[0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.streaming.OutputMode has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(wait,[Default]), UsedName(org;apache;spark;sql;streaming;OutputMode;init;,[Default]), UsedName(Update,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(Append,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(OutputMode,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]), UsedName(Complete,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.streaming.OutputMode: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.streaming.OutputMode.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [Append, OutputMode, hashCode][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.Write has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(toBatch,[Default]), UsedName(toStreaming,[Default]), UsedName(Write,[Default]), UsedName(supportedCustomMetrics,[Default]), UsedName(description,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.Write: Set(org.apache.spark.sql.kafka010.KafkaWrite)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaWrite...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaWrite[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaWrite)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [Write][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.Write.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaWrite: [Write][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [Write][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [Write][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.catalog.SupportsRead has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(properties,[Default]), UsedName(newScanBuilder,[Default]), UsedName(partitioning,[Default]), UsedName(capabilities,[Default]), UsedName(schema,[Default]), UsedName(SupportsRead,[Default]), UsedName(name,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.catalog.SupportsRead: Set(org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.catalog.SupportsRead.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [schema, SupportsRead][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [schema, SupportsRead][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.Scan has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(readSchema,[Default]), UsedName(toBatch,[Default]), UsedName(Scan,[Default]), UsedName(toContinuousStream,[Default]), UsedName(supportedCustomMetrics,[Default]), UsedName(description,[Default]), UsedName(toMicroBatchStream,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.Scan: Set(org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.Scan.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [Scan][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan: [Scan][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.WriteBuilder has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(WriteBuilder,[Default]), UsedName(buildForStreaming,[Default]), UsedName(build,[Default]), UsedName(buildForBatch,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.WriteBuilder: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.WriteBuilder.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [build, WriteBuilder][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [WriteBuilder][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.PartitionReader has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(currentMetricsValues,[Default]), UsedName(close,[Default]), UsedName(PartitionReader,[Default]), UsedName(get,[Default]), UsedName(next,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.PartitionReader: Set(org.apache.spark.sql.kafka010.KafkaBatchPartitionReader)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaBatchPartitionReader...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaBatchPartitionReader[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaBatchPartitionReader)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.PartitionReader.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchInputPartition: [PartitionReader][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchPartitionReader: [PartitionReader, get][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchReaderFactory: [PartitionReader][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.PartitionReaderFactory has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(supportColumnarReads,[Default]), UsedName(PartitionReaderFactory,[Default]), UsedName(createReader,[Default]), UsedName(createColumnarReader,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.PartitionReaderFactory: Set(org.apache.spark.sql.kafka010.KafkaBatchReaderFactory)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaBatchReaderFactory...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaBatchReaderFactory[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaBatchReaderFactory)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatch: [PartitionReaderFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [PartitionReaderFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.PartitionReaderFactory.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchInputPartition: [PartitionReaderFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatch: [PartitionReaderFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchReaderFactory: [PartitionReaderFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [PartitionReaderFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.catalyst.expressions.UnsafeRow has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(setShort,[Default]), UsedName(setTotalSize,[Default]), UsedName(getBoolean,[Default]), UsedName(wait,[Default]), UsedName(setNotNullAt,[Default]), UsedName(writeExternal,[Default]), UsedName(getInterval,[Default]), UsedName(getDecimal,[Default]), UsedName(empty,[Default]), UsedName(getWriter,[Default]), UsedName(getSizeInBytes,[Default]), UsedName(apply,[Default]), UsedName(numFields,[Default]), UsedName(getInt,[Default]), UsedName(writeToMemory,[Default]), UsedName(write,[Default]), UsedName(setFloat,[Default]), UsedName(setBoolean,[Default]), UsedName(copyFrom,[Default]), UsedName(getByte,[Default]), UsedName(copy,[Default]), UsedName(setNullAt,[Default]), UsedName(mutableFieldTypes,[Default]), UsedName(notify,[Default]), UsedName(UnsafeRow,[Default]), UsedName(equals,[Default]), UsedName(isFixedLength,[Default]), UsedName(getShort,[Default]), UsedName(setDecimal,[Default]), UsedName(setDouble,[Default]), UsedName(getAccessor,[Default]), UsedName(org;apache;spark;sql;catalyst;expressions;UnsafeRow;init;,[Default]), UsedName(getLong,[Default]), UsedName(getMap,[Default]), UsedName(WORD_SIZE,[Default]), UsedName(copyValue,[Default]), UsedName(getUTF8String,[Default]), UsedName(createFromByteArray,[Default]), UsedName(setByte,[Default]), UsedName(read,[Default]), UsedName(getBaseObject,[Default]), UsedName(getDouble,[Default]), UsedName(getArray,[Default]), UsedName(getString,[Default]), UsedName(hashCode,[Default]), UsedName(getBinary,[Default]), UsedName(calculateBitSetWidthInBytes,[Default]), UsedName(getFloat,[Default]), UsedName(toSeq,[Default]), UsedName(getAccessor$default$2,[Default]), UsedName(getClass,[Default]), UsedName(isMutable,[Default]), UsedName(update,[Default]), UsedName(writeToStream,[Default]), UsedName($assertionsDisabled,[Default]), UsedName(anyNull,[Default]), UsedName(setLong,[Default]), UsedName(getBytes,[Default]), UsedName(pointTo,[Default]), UsedName(notifyAll,[Default]), UsedName(writeTo,[Default]), UsedName(setInt,[Default]), UsedName(fromSeq,[Default]), UsedName(getBaseOffset,[Default]), UsedName(get,[Default]), UsedName($anonfun$toSeq$1,[Default]), UsedName(getStruct,[Default]), UsedName(toString,[Default]), UsedName(setInterval,[Default]), UsedName(isNullAt,[Default]), UsedName(readExternal,[Default]), UsedName(writeFieldTo,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.catalyst.expressions.UnsafeRow: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.catalyst.expressions.UnsafeRow.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousPartitionReader: [UnsafeRow, apply, read, get][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaRecordToRowConverter: [UnsafeRow, apply][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchInputPartition: [UnsafeRow, apply, toString, read][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchPartitionReader: [UnsafeRow, apply, toString, read, get][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousStream: [UnsafeRow, apply, getLong, toSeq, read, getBoolean][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaRowWriter: [getInt, getStruct, UnsafeRow, getUTF8String, apply, toString, isNullAt, getArray, getBinary][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.streaming.StreamingDataWriterFactory has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(createWriter,[Default]), UsedName(StreamingDataWriterFactory,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.streaming.StreamingDataWriterFactory: Set(org.apache.spark.sql.kafka010.KafkaStreamWriterFactory)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaStreamWriterFactory...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaStreamWriterFactory[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaStreamWriterFactory)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.streaming.StreamingDataWriterFactory.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaStreamWriterFactory: [StreamingDataWriterFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaStreamingWrite: [StreamingDataWriterFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.streaming.StreamingWrite has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(commit,[Default]), UsedName(createStreamingWriterFactory,[Default]), UsedName(abort,[Default]), UsedName(StreamingWrite,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.streaming.StreamingWrite: Set(org.apache.spark.sql.kafka010.KafkaStreamingWrite)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaStreamingWrite...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaStreamingWrite[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaStreamingWrite)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaWrite: [StreamingWrite][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.streaming.StreamingWrite.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaWrite: [StreamingWrite][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaStreamingWrite: [StreamingWrite][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.ReadMinRows has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(org;apache;spark;sql;connector;read;streaming;ReadMinRows;init;,[Default]), UsedName(wait,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(maxTriggerDelayMs,[Default]), UsedName(minRows,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]), UsedName(ReadMinRows,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.ReadMinRows: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.ReadMinRows.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSource: [maxTriggerDelayMs, minRows, toString, ReadMinRows][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [maxTriggerDelayMs, minRows, toString, ReadMinRows][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.ContinuousStream has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(createContinuousReaderFactory,[Default]), UsedName(mergeOffsets,[Default]), UsedName(initialOffset,[Default]), UsedName(planInputPartitions,[Default]), UsedName(needsReconfiguration,[Default]), UsedName(ContinuousStream,[Default]), UsedName(deserializeOffset,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.ContinuousStream: Set(org.apache.spark.sql.kafka010.KafkaContinuousStream)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaContinuousStream...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaContinuousStream[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaContinuousStream)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan: [ContinuousStream][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.ContinuousStream.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [ContinuousStream][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousStream: [ContinuousStream][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan: [ContinuousStream][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.metric.CustomMetric has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(initialValue,[Default]), UsedName(CustomMetric,[Default]), UsedName(aggregateTaskMetrics,[Default]), UsedName(description,[Default]), UsedName(name,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.metric.CustomMetric: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.metric.CustomMetric.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [CustomMetric][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan: [CustomMetric][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.catalog.Table has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(properties,[Default]), UsedName(Table,[Default]), UsedName(partitioning,[Default]), UsedName(capabilities,[Default]), UsedName(schema,[Default]), UsedName(name,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.catalog.Table: Set(org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.catalog.Table.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [Table, schema][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [Table, schema][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.util.CaseInsensitiveStringMap has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(org;apache;spark;sql;util;CaseInsensitiveStringMap;init;,[Default]), UsedName(entrySet,[Default]), UsedName(containsValue,[Default]), UsedName(getBoolean,[Default]), UsedName(replaceAll,[Default]), UsedName(wait,[Default]), UsedName(forEach,[Default]), UsedName(asCaseSensitiveMap,[Default]), UsedName(replace,[Default]), UsedName(empty,[Default]), UsedName(computeIfPresent,[Default]), UsedName(getInt,[Default]), UsedName(getOrDefault,[Default]), UsedName(containsKey,[Default]), UsedName(computeIfAbsent,[Default]), UsedName(putAll,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(remove,[Default]), UsedName(put,[Default]), UsedName(getLong,[Default]), UsedName(values,[Default]), UsedName(size,[Default]), UsedName(getDouble,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(putIfAbsent,[Default]), UsedName(compute,[Default]), UsedName(notifyAll,[Default]), UsedName(CaseInsensitiveStringMap,[Default]), UsedName(isEmpty,[Default]), UsedName(merge,[Default]), UsedName(get,[Default]), UsedName(clear,[Default]), UsedName(toString,[Default]), UsedName(keySet,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.util.CaseInsensitiveStringMap: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.util.CaseInsensitiveStringMap.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaOffsetRangeCalculator: [size, empty, hashCode, CaseInsensitiveStringMap, get, isEmpty][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [keySet, size, hashCode, CaseInsensitiveStringMap, getBoolean, get, isEmpty][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousStream: [containsKey, keySet, getLong, CaseInsensitiveStringMap, getBoolean][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [CaseInsensitiveStringMap, get][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [containsKey, keySet, toString, size, getLong, values, CaseInsensitiveStringMap, getBoolean, get][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan: [CaseInsensitiveStringMap, getBoolean][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.ReadMaxRows has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(wait,[Default]), UsedName(ReadMaxRows,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(maxRows,[Default]), UsedName(notifyAll,[Default]), UsedName(org;apache;spark;sql;connector;read;streaming;ReadMaxRows;init;,[Default]), UsedName(toString,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.ReadMaxRows: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.ReadMaxRows.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSource: [maxRows, toString, ReadMaxRows][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [maxRows, toString, ReadMaxRows][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.ReadAllAvailable has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(wait,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(ReadAllAvailable,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(INSTANCE,[Default]), UsedName(toString,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.ReadAllAvailable: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.ReadAllAvailable.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSource: [ReadAllAvailable, toString][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [ReadAllAvailable, toString][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.DataWriter has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(commit,[Default]), UsedName(currentMetricsValues,[Default]), UsedName(write,[Default]), UsedName(close,[Default]), UsedName(DataWriter,[Default]), UsedName(abort,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.DataWriter: Set(org.apache.spark.sql.kafka010.KafkaDataWriter)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaDataWriter...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaDataWriter[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaDataWriter)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaStreamWriterFactory: [DataWriter, write][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchWriterFactory: [DataWriter, write][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.DataWriter.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaStreamWriterFactory: [DataWriter, write][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaDataWriterCommitMessage: [DataWriter, write][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchWrite: [DataWriter, write][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchWriterFactory: [DataWriter, write][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaStreamingWrite: [DataWriter, write][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaDataWriter: [DataWriter, write][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.internal.SQLConf has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(sparkCostRecordEnabled,[Default]), UsedName(LAST_LEVEL_INDEX,[Default]), UsedName(tableNumLow,[Default]), UsedName(tableNumUp,[Default]), UsedName(LAST_LEVEL_INDEX_ENABLED,[Default]), UsedName(PLAN_GROUP_ENABLED,[Default]), UsedName(LAST_LEVEL_EPSILON_ENABLED,[Default]), UsedName(lastLevelIndexEnabled,[Default]), UsedName(lastLevelSize,[Default]), UsedName(lastLevelEpsilonEnabled,[Default]), UsedName(SPARK_COST_SIZE,[Default]), UsedName(LAST_LEVEL_EPSILON,[Default]), UsedName(lastLevelEpsilon,[Default]), UsedName(maxLastLevel,[Default]), UsedName(TABLE_NUM_UP,[Default]), UsedName(PLAN_GROUP_SIZE,[Default]), UsedName(sparkCostCard,[Default]), UsedName(planGroupEnabled,[Default]), UsedName(lastLevelSizeEnabled,[Default]), UsedName(TABLE_NUM_LOW,[Default]), UsedName(LAST_LEVEL_SIZE,[Default]), UsedName(LAST_LEVEL_SIZE_ENABLED,[Default]), UsedName(maxLastLevelEnabled,[Default]), UsedName(MAX_LAST_LEVEL_ENABLED,[Default]), UsedName(sparkCostSize,[Default]), UsedName(planGroupSize,[Default]), UsedName(SPARK_COST_CARD,[Default]), UsedName(SPARK_COST_RECORD_ENABLED,[Default]), UsedName(MAX_LAST_LEVEL,[Default]), UsedName(lastLevelIndex,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.internal.SQLConf: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.internal.SQLConf.[0m
[0m[[0m[0mdebug[0m] [0m[0mNone of the modified names appears in source file of org.apache.spark.sql.kafka010.KafkaOffsetReader. This dependency is not being considered for invalidation.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.CompositeReadLimit has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(getReadLimits,[Default]), UsedName(wait,[Default]), UsedName(org;apache;spark;sql;connector;read;streaming;CompositeReadLimit;init;,[Default]), UsedName(CompositeReadLimit,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.CompositeReadLimit: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.CompositeReadLimit.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSource: [CompositeReadLimit, getReadLimits, toString][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [CompositeReadLimit, getReadLimits, toString][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.Batch has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(planInputPartitions,[Default]), UsedName(createReaderFactory,[Default]), UsedName(Batch,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.Batch: Set(org.apache.spark.sql.kafka010.KafkaBatch)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaBatch...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaBatch[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaBatch)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan: [Batch][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.Batch.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatch: [Batch][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [Batch][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan: [Batch][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.catalog.SupportsWrite has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(newWriteBuilder,[Default]), UsedName(properties,[Default]), UsedName(partitioning,[Default]), UsedName(capabilities,[Default]), UsedName(SupportsWrite,[Default]), UsedName(schema,[Default]), UsedName(name,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.catalog.SupportsWrite: Set(org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.catalog.SupportsWrite.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [SupportsWrite, schema][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [SupportsWrite, schema][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.PartitionOffset has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(PartitionOffset,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.PartitionOffset: Set(org.apache.spark.sql.kafka010.KafkaSourcePartitionOffset)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaSourcePartitionOffset...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaSourcePartitionOffset[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaSourcePartitionOffset)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mNone of the modified names appears in source file of org.apache.spark.sql.kafka010.KafkaContinuousPartitionReader. This dependency is not being considered for invalidation.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousStream: [PartitionOffset][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.PartitionOffset.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourcePartitionOffset: [PartitionOffset][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceOffset: [PartitionOffset][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousStream: [PartitionOffset][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.metric.CustomTaskMetric has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(name,[Default]), UsedName(CustomTaskMetric,[Default]), UsedName(value,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.metric.CustomTaskMetric: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.metric.CustomTaskMetric.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchInputPartition: [CustomTaskMetric][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchPartitionReader: [CustomTaskMetric][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.WriterCommitMessage has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(WriterCommitMessage,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.WriterCommitMessage: Set(org.apache.spark.sql.kafka010.KafkaDataWriterCommitMessage)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaDataWriterCommitMessage...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaDataWriterCommitMessage[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaDataWriterCommitMessage)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.WriterCommitMessage.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaDataWriterCommitMessage: [WriterCommitMessage][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchWrite: [WriterCommitMessage][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaStreamingWrite: [WriterCommitMessage][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaDataWriter: [WriterCommitMessage][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.MicroBatchStream has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(latestOffset,[Default]), UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(MicroBatchStream,[Default]), UsedName(initialOffset,[Default]), UsedName(planInputPartitions,[Default]), UsedName(deserializeOffset,[Default]), UsedName(createReaderFactory,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.MicroBatchStream: Set(org.apache.spark.sql.kafka010.KafkaMicroBatchStream)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaMicroBatchStream...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaMicroBatchStream[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaMicroBatchStream)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan: [MicroBatchStream][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.MicroBatchStream.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [MicroBatchStream][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [MicroBatchStream][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan: [MicroBatchStream][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.Offset has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(json,[Default]), UsedName(wait,[Default]), UsedName(Offset,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(org;apache;spark;sql;connector;read;streaming;Offset;init;,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.Offset: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.Offset.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceOffset: [getClass, toString, json, Offset][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousStream: [json, Offset][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSource: [toString, Offset][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [toString, json, Offset][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.SupportsTruncate has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(truncate,[Default]), UsedName(buildForStreaming,[Default]), UsedName(SupportsTruncate,[Default]), UsedName(build,[Default]), UsedName(buildForBatch,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.SupportsTruncate: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.SupportsTruncate.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [build, SupportsTruncate][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [SupportsTruncate][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.internal.SQLConf$ has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(sparkCostRecordEnabled,[Default]), UsedName(LAST_LEVEL_INDEX,[Default]), UsedName(tableNumLow,[Default]), UsedName(tableNumUp,[Default]), UsedName(LAST_LEVEL_INDEX_ENABLED,[Default]), UsedName(PLAN_GROUP_ENABLED,[Default]), UsedName(LAST_LEVEL_EPSILON_ENABLED,[Default]), UsedName(lastLevelIndexEnabled,[Default]), UsedName(lastLevelSize,[Default]), UsedName(lastLevelEpsilonEnabled,[Default]), UsedName(SPARK_COST_SIZE,[Default]), UsedName(LAST_LEVEL_EPSILON,[Default]), UsedName(lastLevelEpsilon,[Default]), UsedName(maxLastLevel,[Default]), UsedName(TABLE_NUM_UP,[Default]), UsedName(PLAN_GROUP_SIZE,[Default]), UsedName(sparkCostCard,[Default]), UsedName(planGroupEnabled,[Default]), UsedName(lastLevelSizeEnabled,[Default]), UsedName(TABLE_NUM_LOW,[Default]), UsedName(LAST_LEVEL_SIZE,[Default]), UsedName(LAST_LEVEL_SIZE_ENABLED,[Default]), UsedName(maxLastLevelEnabled,[Default]), UsedName(MAX_LAST_LEVEL_ENABLED,[Default]), UsedName(sparkCostSize,[Default]), UsedName(planGroupSize,[Default]), UsedName(SPARK_COST_CARD,[Default]), UsedName(SPARK_COST_RECORD_ENABLED,[Default]), UsedName(MAX_LAST_LEVEL,[Default]), UsedName(lastLevelIndex,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.internal.SQLConf$: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.internal.SQLConf$.[0m
[0m[[0m[0mdebug[0m] [0m[0mNone of the modified names appears in source file of org.apache.spark.sql.kafka010.KafkaOffsetReader. This dependency is not being considered for invalidation.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.PhysicalWriteInfo has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(numPartitions,[Default]), UsedName(PhysicalWriteInfo,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.PhysicalWriteInfo: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.PhysicalWriteInfo.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchWrite: [PhysicalWriteInfo][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaStreamingWrite: [PhysicalWriteInfo][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.ScanBuilder has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(ScanBuilder,[Default]), UsedName(build,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.ScanBuilder: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.ScanBuilder.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [ScanBuilder, build][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [ScanBuilder][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.catalog.TableCapability has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(compareTo,[Default]), UsedName(wait,[Default]), UsedName(OVERWRITE_BY_FILTER,[Default]), UsedName(V1_BATCH_WRITE,[Default]), UsedName(MICRO_BATCH_READ,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(TableCapability,[Default]), UsedName(ordinal,[Default]), UsedName(CONTINUOUS_READ,[Default]), UsedName(STREAMING_WRITE,[Default]), UsedName(values,[Default]), UsedName(ACCEPT_ANY_SCHEMA,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(BATCH_WRITE,[Default]), UsedName(BATCH_READ,[Default]), UsedName(OVERWRITE_DYNAMIC,[Default]), UsedName(notifyAll,[Default]), UsedName(name,[Default]), UsedName(getDeclaringClass,[Default]), UsedName(TRUNCATE,[Default]), UsedName(valueOf,[Default]), UsedName(toString,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.catalog.TableCapability: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.catalog.TableCapability.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [valueOf, TableCapability, hashCode][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [CONTINUOUS_READ, ACCEPT_ANY_SCHEMA, MICRO_BATCH_READ, BATCH_WRITE, STREAMING_WRITE, TableCapability, BATCH_READ][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.BatchWrite has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(commit,[Default]), UsedName(BatchWrite,[Default]), UsedName(onDataWriterCommit,[Default]), UsedName(createBatchWriterFactory,[Default]), UsedName(useCommitCoordinator,[Default]), UsedName(abort,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.BatchWrite: Set(org.apache.spark.sql.kafka010.KafkaBatchWrite)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaBatchWrite...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaBatchWrite[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaBatchWrite)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaWrite: [BatchWrite][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.BatchWrite.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaWrite: [BatchWrite][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchWrite: [BatchWrite][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReaderFactory has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(supportColumnarReads,[Default]), UsedName(ContinuousPartitionReaderFactory,[Default]), UsedName(createReader,[Default]), UsedName(createColumnarReader,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReaderFactory: Set(org.apache.spark.sql.kafka010.KafkaContinuousReaderFactory)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaContinuousReaderFactory...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaContinuousReaderFactory[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaContinuousReaderFactory)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReaderFactory.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousStream: [ContinuousPartitionReaderFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousReaderFactory: [ContinuousPartitionReaderFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.InputPartition has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(InputPartition,[Default]), UsedName(preferredLocations,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.InputPartition: Set(org.apache.spark.sql.kafka010.KafkaBatchInputPartition, org.apache.spark.sql.kafka010.KafkaContinuousInputPartition)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaBatchInputPartition...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaBatchInputPartition[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaBatchInputPartition)[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaContinuousInputPartition...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaContinuousInputPartition[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaContinuousInputPartition)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatch: [InputPartition][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [InputPartition][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.InputPartition.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchInputPartition: [InputPartition][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatch: [InputPartition][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchReaderFactory: [InputPartition][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousStream: [InputPartition][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [InputPartition][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousReaderFactory: [InputPartition][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousInputPartition: [InputPartition][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.DataWriterFactory has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(createWriter,[Default]), UsedName(DataWriterFactory,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.DataWriterFactory: Set(org.apache.spark.sql.kafka010.KafkaBatchWriterFactory)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaBatchWriterFactory...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaBatchWriterFactory[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaBatchWriterFactory)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.DataWriterFactory.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchWrite: [DataWriterFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaBatchWriterFactory: [DataWriterFactory][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.ReportsSourceMetrics has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(ReportsSourceMetrics,[Default]), UsedName(initialOffset,[Default]), UsedName(deserializeOffset,[Default]), UsedName(metrics,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.ReportsSourceMetrics: Set(org.apache.spark.sql.kafka010.KafkaMicroBatchStream)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaMicroBatchStream...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaMicroBatchStream[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaMicroBatchStream)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mNone of the modified names appears in source file of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan. This dependency is not being considered for invalidation.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.ReportsSourceMetrics.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [ReportsSourceMetrics, metrics][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.execution.streaming.Offset has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(json,[Default]), UsedName(wait,[Default]), UsedName(Offset,[Default]), UsedName(org;apache;spark;sql;execution;streaming;Offset;init;,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.execution.streaming.Offset: Set(org.apache.spark.sql.kafka010.KafkaSourceOffset)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaSourceOffset...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaSourceOffset[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaSourceOffset)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin: [toString][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousStream: [json, Offset][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSource: [toString, Offset][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [toString, json, Offset][0m
[0m[[0m[0mdebug[0m] [0m[0mNone of the modified names appears in source file of org.apache.spark.sql.kafka010.KafkaOffsetReader. This dependency is not being considered for invalidation.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer: [toString][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceInitialOffsetWriter: [toString, json][0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.execution.streaming.Offset.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceOffset: [getClass, toString, json, Offset, org;apache;spark;sql;execution;streaming;Offset;init;][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSource: [toString, Offset][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.SupportsTriggerAvailableNow has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(latestOffset,[Default]), UsedName(reportLatestOffset,[Default]), UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(getDefaultReadLimit,[Default]), UsedName(initialOffset,[Default]), UsedName(prepareForTriggerAvailableNow,[Default]), UsedName(SupportsTriggerAvailableNow,[Default]), UsedName(deserializeOffset,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.SupportsTriggerAvailableNow: Set(org.apache.spark.sql.kafka010.KafkaSource, org.apache.spark.sql.kafka010.KafkaMicroBatchStream)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaSource...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaSource[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaSource)[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaMicroBatchStream...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaMicroBatchStream[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaMicroBatchStream)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mNone of the modified names appears in source file of org.apache.spark.sql.kafka010.KafkaSourceProvider. This dependency is not being considered for invalidation.[0m
[0m[[0m[0mdebug[0m] [0m[0mNone of the modified names appears in source file of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan. This dependency is not being considered for invalidation.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.SupportsTriggerAvailableNow.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSource: [SupportsTriggerAvailableNow][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [SupportsTriggerAvailableNow][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.SaveMode has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(compareTo,[Default]), UsedName(wait,[Default]), UsedName(ErrorIfExists,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(ordinal,[Default]), UsedName(Overwrite,[Default]), UsedName(values,[Default]), UsedName(Append,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(name,[Default]), UsedName(SaveMode,[Default]), UsedName(getDeclaringClass,[Default]), UsedName(valueOf,[Default]), UsedName(toString,[Default]), UsedName(Ignore,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.SaveMode: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.SaveMode.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [valueOf, SaveMode, Append, ErrorIfExists, Ignore, Overwrite, hashCode, SaveMode][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.metric.CustomSumMetric has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(wait,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(initialValue,[Default]), UsedName(org;apache;spark;sql;connector;metric;CustomSumMetric;init;,[Default]), UsedName(CustomSumMetric,[Default]), UsedName(hashCode,[Default]), UsedName(aggregateTaskMetrics,[Default]), UsedName(getClass,[Default]), UsedName(description,[Default]), UsedName(notifyAll,[Default]), UsedName(name,[Default]), UsedName(toString,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.metric.CustomSumMetric: Set(org.apache.spark.sql.kafka010.OffsetOutOfRangeMetric, org.apache.spark.sql.kafka010.DataLossMetric)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.OffsetOutOfRangeMetric...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.OffsetOutOfRangeMetric[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.OffsetOutOfRangeMetric)[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.DataLossMetric...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.DataLossMetric[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.DataLossMetric)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.metric.CustomSumMetric.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.OffsetOutOfRangeMetric: [org;apache;spark;sql;connector;metric;CustomSumMetric;init;, CustomSumMetric][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [CustomSumMetric, hashCode][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.DataLossMetric: [org;apache;spark;sql;connector;metric;CustomSumMetric;init;, CustomSumMetric][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.write.LogicalWriteInfo has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(schema,[Default]), UsedName(options,[Default]), UsedName(LogicalWriteInfo,[Default]), UsedName(queryId,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.write.LogicalWriteInfo: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.write.LogicalWriteInfo.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider: [options, LogicalWriteInfo, schema][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable: [options, LogicalWriteInfo, schema][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(currentMetricsValues,[Default]), UsedName(close,[Default]), UsedName(ContinuousPartitionReader,[Default]), UsedName(getOffset,[Default]), UsedName(get,[Default]), UsedName(next,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader: Set(org.apache.spark.sql.kafka010.KafkaContinuousPartitionReader)[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidating (transitively) by inheritance from org.apache.spark.sql.kafka010.KafkaContinuousPartitionReader...[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaContinuousPartitionReader[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated by transitive inheritance dependency: Set(org.apache.spark.sql.kafka010.KafkaContinuousPartitionReader)[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousPartitionReader: [ContinuousPartitionReader, get, next][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousStream: [ContinuousPartitionReader, close][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaContinuousReaderFactory: [ContinuousPartitionReader][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.catalyst.expressions.UnsafeArrayData has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(calculateHeaderPortionInBytes,[Default]), UsedName(toObjectArray,[Default]), UsedName(setShort,[Default]), UsedName(UnsafeArrayData,[Default]), UsedName(getBoolean,[Default]), UsedName(wait,[Default]), UsedName(numElements,[Default]), UsedName(toIntArray,[Default]), UsedName(writeExternal,[Default]), UsedName(getInterval,[Default]), UsedName(getDecimal,[Default]), UsedName(getSizeInBytes,[Default]), UsedName(createFreshArray,[Default]), UsedName(getInt,[Default]), UsedName(writeToMemory,[Default]), UsedName(write,[Default]), UsedName(setFloat,[Default]), UsedName(setBoolean,[Default]), UsedName(toByteArray,[Default]), UsedName(getByte,[Default]), UsedName(allocateArrayData,[Default]), UsedName(copy,[Default]), UsedName(setNullAt,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(toLongArray,[Default]), UsedName(toDoubleArray,[Default]), UsedName(getShort,[Default]), UsedName(toBooleanArray,[Default]), UsedName(setDouble,[Default]), UsedName(getLong,[Default]), UsedName(getMap,[Default]), UsedName(getUTF8String,[Default]), UsedName(setByte,[Default]), UsedName(read,[Default]), UsedName(toShortArray,[Default]), UsedName(toFloatArray,[Default]), UsedName(foreach,[Default]), UsedName(getBaseObject,[Default]), UsedName(getDouble,[Default]), UsedName(getArray,[Default]), UsedName(hashCode,[Default]), UsedName(getBinary,[Default]), UsedName(toArrayData,[Default]), UsedName(getFloat,[Default]), UsedName(toSeq,[Default]), UsedName(getClass,[Default]), UsedName(update,[Default]), UsedName($assertionsDisabled,[Default]), UsedName(setLong,[Default]), UsedName(fromPrimitiveArray,[Default]), UsedName(pointTo,[Default]), UsedName(notifyAll,[Default]), UsedName(writeTo,[Default]), UsedName(setInt,[Default]), UsedName(org;apache;spark;sql;catalyst;expressions;UnsafeArrayData;init;,[Default]), UsedName(getBaseOffset,[Default]), UsedName(get,[Default]), UsedName(getStruct,[Default]), UsedName(toString,[Default]), UsedName(shouldUseGenericArrayData,[Default]), UsedName(toArray,[Default]), UsedName(isNullAt,[Default]), UsedName(array,[Default]), UsedName(readExternal,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.catalyst.expressions.UnsafeArrayData: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.catalyst.expressions.UnsafeArrayData.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaRowWriter: [getInt, getStruct, getUTF8String, toString, numElements, isNullAt, getArray, getBinary, UnsafeArrayData][0m
[0m[[0m[0mdebug[0m] [0m[0mThe org.apache.spark.sql.connector.read.streaming.ReadLimit has the following regular definitions changed:[0m
[0m[[0m[0mdebug[0m] [0m[0m	UsedName(maxFiles,[Default]), UsedName(allAvailable,[Default]), UsedName(compositeLimit,[Default]), UsedName(maxRows,[Default]), UsedName(minRows,[Default]), UsedName(ReadLimit,[Default]).[0m
[0m[[0m[0mdebug[0m] [0m[0mAll member reference dependencies will be considered within this context.[0m
[0m[[0m[0mdebug[0m] [0m[0mFiles invalidated by inheriting from (external) org.apache.spark.sql.connector.read.streaming.ReadLimit: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mNow invalidating by inheritance (internally).[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting direct dependencies of all classes transitively invalidated by inheritance.[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting classes that directly depend on (external) org.apache.spark.sql.connector.read.streaming.ReadLimit.[0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaSource: [maxRows, ReadLimit, minRows, allAvailable, compositeLimit][0m
[0m[[0m[0mdebug[0m] [0m[0mThe following modified names cause invalidation of org.apache.spark.sql.kafka010.KafkaMicroBatchStream: [maxRows, ReadLimit, minRows, allAvailable, compositeLimit][0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial source changes:[0m
[0m[[0m[0mdebug[0m] [0m[0m	removed: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0m	added: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0m	modified: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mInvalidated products: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mExternal API changes: API Changes: Set(NamesChange(org.apache.spark.sql.streaming.OutputMode,ModifiedNames(changes = UsedName(wait,[Default]), UsedName(org;apache;spark;sql;streaming;OutputMode;init;,[Default]), UsedName(Update,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(Append,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(OutputMode,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]), UsedName(Complete,[Default]))), NamesChange(org.apache.spark.sql.connector.write.Write,ModifiedNames(changes = UsedName(toBatch,[Default]), UsedName(toStreaming,[Default]), UsedName(Write,[Default]), UsedName(supportedCustomMetrics,[Default]), UsedName(description,[Default]))), NamesChange(org.apache.spark.sql.connector.catalog.SupportsRead,ModifiedNames(changes = UsedName(properties,[Default]), UsedName(newScanBuilder,[Default]), UsedName(partitioning,[Default]), UsedName(capabilities,[Default]), UsedName(schema,[Default]), UsedName(SupportsRead,[Default]), UsedName(name,[Default]))), NamesChange(org.apache.spark.sql.connector.read.Scan,ModifiedNames(changes = UsedName(readSchema,[Default]), UsedName(toBatch,[Default]), UsedName(Scan,[Default]), UsedName(toContinuousStream,[Default]), UsedName(supportedCustomMetrics,[Default]), UsedName(description,[Default]), UsedName(toMicroBatchStream,[Default]))), NamesChange(org.apache.spark.sql.connector.write.WriteBuilder,ModifiedNames(changes = UsedName(WriteBuilder,[Default]), UsedName(buildForStreaming,[Default]), UsedName(build,[Default]), UsedName(buildForBatch,[Default]))), NamesChange(org.apache.spark.sql.connector.read.PartitionReader,ModifiedNames(changes = UsedName(currentMetricsValues,[Default]), UsedName(close,[Default]), UsedName(PartitionReader,[Default]), UsedName(get,[Default]), UsedName(next,[Default]))), NamesChange(org.apache.spark.sql.connector.read.PartitionReaderFactory,ModifiedNames(changes = UsedName(supportColumnarReads,[Default]), UsedName(PartitionReaderFactory,[Default]), UsedName(createReader,[Default]), UsedName(createColumnarReader,[Default]))), NamesChange(org.apache.spark.sql.catalyst.expressions.UnsafeRow,ModifiedNames(changes = UsedName(setShort,[Default]), UsedName(setTotalSize,[Default]), UsedName(getBoolean,[Default]), UsedName(wait,[Default]), UsedName(setNotNullAt,[Default]), UsedName(writeExternal,[Default]), UsedName(getInterval,[Default]), UsedName(getDecimal,[Default]), UsedName(empty,[Default]), UsedName(getWriter,[Default]), UsedName(getSizeInBytes,[Default]), UsedName(apply,[Default]), UsedName(numFields,[Default]), UsedName(getInt,[Default]), UsedName(writeToMemory,[Default]), UsedName(write,[Default]), UsedName(setFloat,[Default]), UsedName(setBoolean,[Default]), UsedName(copyFrom,[Default]), UsedName(getByte,[Default]), UsedName(copy,[Default]), UsedName(setNullAt,[Default]), UsedName(mutableFieldTypes,[Default]), UsedName(notify,[Default]), UsedName(UnsafeRow,[Default]), UsedName(equals,[Default]), UsedName(isFixedLength,[Default]), UsedName(getShort,[Default]), UsedName(setDecimal,[Default]), UsedName(setDouble,[Default]), UsedName(getAccessor,[Default]), UsedName(org;apache;spark;sql;catalyst;expressions;UnsafeRow;init;,[Default]), UsedName(getLong,[Default]), UsedName(getMap,[Default]), UsedName(WORD_SIZE,[Default]), UsedName(copyValue,[Default]), UsedName(getUTF8String,[Default]), UsedName(createFromByteArray,[Default]), UsedName(setByte,[Default]), UsedName(read,[Default]), UsedName(getBaseObject,[Default]), UsedName(getDouble,[Default]), UsedName(getArray,[Default]), UsedName(getString,[Default]), UsedName(hashCode,[Default]), UsedName(getBinary,[Default]), UsedName(calculateBitSetWidthInBytes,[Default]), UsedName(getFloat,[Default]), UsedName(toSeq,[Default]), UsedName(getAccessor$default$2,[Default]), UsedName(getClass,[Default]), UsedName(isMutable,[Default]), UsedName(update,[Default]), UsedName(writeToStream,[Default]), UsedName($assertionsDisabled,[Default]), UsedName(anyNull,[Default]), UsedName(setLong,[Default]), UsedName(getBytes,[Default]), UsedName(pointTo,[Default]), UsedName(notifyAll,[Default]), UsedName(writeTo,[Default]), UsedName(setInt,[Default]), UsedName(fromSeq,[Default]), UsedName(getBaseOffset,[Default]), UsedName(get,[Default]), UsedName($anonfun$toSeq$1,[Default]), UsedName(getStruct,[Default]), UsedName(toString,[Default]), UsedName(setInterval,[Default]), UsedName(isNullAt,[Default]), UsedName(readExternal,[Default]), UsedName(writeFieldTo,[Default]))), NamesChange(org.apache.spark.sql.connector.write.streaming.StreamingDataWriterFactory,ModifiedNames(changes = UsedName(createWriter,[Default]), UsedName(StreamingDataWriterFactory,[Default]))), NamesChange(org.apache.spark.sql.connector.write.streaming.StreamingWrite,ModifiedNames(changes = UsedName(commit,[Default]), UsedName(createStreamingWriterFactory,[Default]), UsedName(abort,[Default]), UsedName(StreamingWrite,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ReadMinRows,ModifiedNames(changes = UsedName(org;apache;spark;sql;connector;read;streaming;ReadMinRows;init;,[Default]), UsedName(wait,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(maxTriggerDelayMs,[Default]), UsedName(minRows,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]), UsedName(ReadMinRows,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ContinuousStream,ModifiedNames(changes = UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(createContinuousReaderFactory,[Default]), UsedName(mergeOffsets,[Default]), UsedName(initialOffset,[Default]), UsedName(planInputPartitions,[Default]), UsedName(needsReconfiguration,[Default]), UsedName(ContinuousStream,[Default]), UsedName(deserializeOffset,[Default]))), NamesChange(org.apache.spark.sql.connector.metric.CustomMetric,ModifiedNames(changes = UsedName(initialValue,[Default]), UsedName(CustomMetric,[Default]), UsedName(aggregateTaskMetrics,[Default]), UsedName(description,[Default]), UsedName(name,[Default]))), NamesChange(org.apache.spark.sql.connector.catalog.Table,ModifiedNames(changes = UsedName(properties,[Default]), UsedName(Table,[Default]), UsedName(partitioning,[Default]), UsedName(capabilities,[Default]), UsedName(schema,[Default]), UsedName(name,[Default]))), NamesChange(org.apache.spark.sql.util.CaseInsensitiveStringMap,ModifiedNames(changes = UsedName(org;apache;spark;sql;util;CaseInsensitiveStringMap;init;,[Default]), UsedName(entrySet,[Default]), UsedName(containsValue,[Default]), UsedName(getBoolean,[Default]), UsedName(replaceAll,[Default]), UsedName(wait,[Default]), UsedName(forEach,[Default]), UsedName(asCaseSensitiveMap,[Default]), UsedName(replace,[Default]), UsedName(empty,[Default]), UsedName(computeIfPresent,[Default]), UsedName(getInt,[Default]), UsedName(getOrDefault,[Default]), UsedName(containsKey,[Default]), UsedName(computeIfAbsent,[Default]), UsedName(putAll,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(remove,[Default]), UsedName(put,[Default]), UsedName(getLong,[Default]), UsedName(values,[Default]), UsedName(size,[Default]), UsedName(getDouble,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(putIfAbsent,[Default]), UsedName(compute,[Default]), UsedName(notifyAll,[Default]), UsedName(CaseInsensitiveStringMap,[Default]), UsedName(isEmpty,[Default]), UsedName(merge,[Default]), UsedName(get,[Default]), UsedName(clear,[Default]), UsedName(toString,[Default]), UsedName(keySet,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ReadMaxRows,ModifiedNames(changes = UsedName(wait,[Default]), UsedName(ReadMaxRows,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(maxRows,[Default]), UsedName(notifyAll,[Default]), UsedName(org;apache;spark;sql;connector;read;streaming;ReadMaxRows;init;,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ReadAllAvailable,ModifiedNames(changes = UsedName(wait,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(ReadAllAvailable,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(INSTANCE,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.write.DataWriter,ModifiedNames(changes = UsedName(commit,[Default]), UsedName(currentMetricsValues,[Default]), UsedName(write,[Default]), UsedName(close,[Default]), UsedName(DataWriter,[Default]), UsedName(abort,[Default]))), NamesChange(org.apache.spark.sql.internal.SQLConf,ModifiedNames(changes = UsedName(sparkCostRecordEnabled,[Default]), UsedName(LAST_LEVEL_INDEX,[Default]), UsedName(tableNumLow,[Default]), UsedName(tableNumUp,[Default]), UsedName(LAST_LEVEL_INDEX_ENABLED,[Default]), UsedName(PLAN_GROUP_ENABLED,[Default]), UsedName(LAST_LEVEL_EPSILON_ENABLED,[Default]), UsedName(lastLevelIndexEnabled,[Default]), UsedName(lastLevelSize,[Default]), UsedName(lastLevelEpsilonEnabled,[Default]), UsedName(SPARK_COST_SIZE,[Default]), UsedName(LAST_LEVEL_EPSILON,[Default]), UsedName(lastLevelEpsilon,[Default]), UsedName(maxLastLevel,[Default]), UsedName(TABLE_NUM_UP,[Default]), UsedName(PLAN_GROUP_SIZE,[Default]), UsedName(sparkCostCard,[Default]), UsedName(planGroupEnabled,[Default]), UsedName(lastLevelSizeEnabled,[Default]), UsedName(TABLE_NUM_LOW,[Default]), UsedName(LAST_LEVEL_SIZE,[Default]), UsedName(LAST_LEVEL_SIZE_ENABLED,[Default]), UsedName(maxLastLevelEnabled,[Default]), UsedName(MAX_LAST_LEVEL_ENABLED,[Default]), UsedName(sparkCostSize,[Default]), UsedName(planGroupSize,[Default]), UsedName(SPARK_COST_CARD,[Default]), UsedName(SPARK_COST_RECORD_ENABLED,[Default]), UsedName(MAX_LAST_LEVEL,[Default]), UsedName(lastLevelIndex,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.CompositeReadLimit,ModifiedNames(changes = UsedName(getReadLimits,[Default]), UsedName(wait,[Default]), UsedName(org;apache;spark;sql;connector;read;streaming;CompositeReadLimit;init;,[Default]), UsedName(CompositeReadLimit,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.read.Batch,ModifiedNames(changes = UsedName(planInputPartitions,[Default]), UsedName(createReaderFactory,[Default]), UsedName(Batch,[Default]))), NamesChange(org.apache.spark.sql.connector.catalog.SupportsWrite,ModifiedNames(changes = UsedName(newWriteBuilder,[Default]), UsedName(properties,[Default]), UsedName(partitioning,[Default]), UsedName(capabilities,[Default]), UsedName(SupportsWrite,[Default]), UsedName(schema,[Default]), UsedName(name,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.PartitionOffset,ModifiedNames(changes = UsedName(PartitionOffset,[Default]))), NamesChange(org.apache.spark.sql.connector.metric.CustomTaskMetric,ModifiedNames(changes = UsedName(name,[Default]), UsedName(CustomTaskMetric,[Default]), UsedName(value,[Default]))), NamesChange(org.apache.spark.sql.connector.write.WriterCommitMessage,ModifiedNames(changes = UsedName(WriterCommitMessage,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.MicroBatchStream,ModifiedNames(changes = UsedName(latestOffset,[Default]), UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(MicroBatchStream,[Default]), UsedName(initialOffset,[Default]), UsedName(planInputPartitions,[Default]), UsedName(deserializeOffset,[Default]), UsedName(createReaderFactory,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.Offset,ModifiedNames(changes = UsedName(json,[Default]), UsedName(wait,[Default]), UsedName(Offset,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(org;apache;spark;sql;connector;read;streaming;Offset;init;,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.write.SupportsTruncate,ModifiedNames(changes = UsedName(truncate,[Default]), UsedName(buildForStreaming,[Default]), UsedName(SupportsTruncate,[Default]), UsedName(build,[Default]), UsedName(buildForBatch,[Default]))), NamesChange(org.apache.spark.sql.internal.SQLConf$,ModifiedNames(changes = UsedName(sparkCostRecordEnabled,[Default]), UsedName(LAST_LEVEL_INDEX,[Default]), UsedName(tableNumLow,[Default]), UsedName(tableNumUp,[Default]), UsedName(LAST_LEVEL_INDEX_ENABLED,[Default]), UsedName(PLAN_GROUP_ENABLED,[Default]), UsedName(LAST_LEVEL_EPSILON_ENABLED,[Default]), UsedName(lastLevelIndexEnabled,[Default]), UsedName(lastLevelSize,[Default]), UsedName(lastLevelEpsilonEnabled,[Default]), UsedName(SPARK_COST_SIZE,[Default]), UsedName(LAST_LEVEL_EPSILON,[Default]), UsedName(lastLevelEpsilon,[Default]), UsedName(maxLastLevel,[Default]), UsedName(TABLE_NUM_UP,[Default]), UsedName(PLAN_GROUP_SIZE,[Default]), UsedName(sparkCostCard,[Default]), UsedName(planGroupEnabled,[Default]), UsedName(lastLevelSizeEnabled,[Default]), UsedName(TABLE_NUM_LOW,[Default]), UsedName(LAST_LEVEL_SIZE,[Default]), UsedName(LAST_LEVEL_SIZE_ENABLED,[Default]), UsedName(maxLastLevelEnabled,[Default]), UsedName(MAX_LAST_LEVEL_ENABLED,[Default]), UsedName(sparkCostSize,[Default]), UsedName(planGroupSize,[Default]), UsedName(SPARK_COST_CARD,[Default]), UsedName(SPARK_COST_RECORD_ENABLED,[Default]), UsedName(MAX_LAST_LEVEL,[Default]), UsedName(lastLevelIndex,[Default]))), NamesChange(org.apache.spark.sql.connector.write.PhysicalWriteInfo,ModifiedNames(changes = UsedName(numPartitions,[Default]), UsedName(PhysicalWriteInfo,[Default]))), NamesChange(org.apache.spark.sql.connector.read.ScanBuilder,ModifiedNames(changes = UsedName(ScanBuilder,[Default]), UsedName(build,[Default]))), NamesChange(org.apache.spark.sql.connector.catalog.TableCapability,ModifiedNames(changes = UsedName(compareTo,[Default]), UsedName(wait,[Default]), UsedName(OVERWRITE_BY_FILTER,[Default]), UsedName(V1_BATCH_WRITE,[Default]), UsedName(MICRO_BATCH_READ,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(TableCapability,[Default]), UsedName(ordinal,[Default]), UsedName(CONTINUOUS_READ,[Default]), UsedName(STREAMING_WRITE,[Default]), UsedName(values,[Default]), UsedName(ACCEPT_ANY_SCHEMA,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(BATCH_WRITE,[Default]), UsedName(BATCH_READ,[Default]), UsedName(OVERWRITE_DYNAMIC,[Default]), UsedName(notifyAll,[Default]), UsedName(name,[Default]), UsedName(getDeclaringClass,[Default]), UsedName(TRUNCATE,[Default]), UsedName(valueOf,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.write.BatchWrite,ModifiedNames(changes = UsedName(commit,[Default]), UsedName(BatchWrite,[Default]), UsedName(onDataWriterCommit,[Default]), UsedName(createBatchWriterFactory,[Default]), UsedName(useCommitCoordinator,[Default]), UsedName(abort,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReaderFactory,ModifiedNames(changes = UsedName(supportColumnarReads,[Default]), UsedName(ContinuousPartitionReaderFactory,[Default]), UsedName(createReader,[Default]), UsedName(createColumnarReader,[Default]))), NamesChange(org.apache.spark.sql.connector.read.InputPartition,ModifiedNames(changes = UsedName(InputPartition,[Default]), UsedName(preferredLocations,[Default]))), NamesChange(org.apache.spark.sql.connector.write.DataWriterFactory,ModifiedNames(changes = UsedName(createWriter,[Default]), UsedName(DataWriterFactory,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ReportsSourceMetrics,ModifiedNames(changes = UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(ReportsSourceMetrics,[Default]), UsedName(initialOffset,[Default]), UsedName(deserializeOffset,[Default]), UsedName(metrics,[Default]))), NamesChange(org.apache.spark.sql.execution.streaming.Offset,ModifiedNames(changes = UsedName(json,[Default]), UsedName(wait,[Default]), UsedName(Offset,[Default]), UsedName(org;apache;spark;sql;execution;streaming;Offset;init;,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.SupportsTriggerAvailableNow,ModifiedNames(changes = UsedName(latestOffset,[Default]), UsedName(reportLatestOffset,[Default]), UsedName(commit,[Default]), UsedName(stop,[Default]), UsedName(getDefaultReadLimit,[Default]), UsedName(initialOffset,[Default]), UsedName(prepareForTriggerAvailableNow,[Default]), UsedName(SupportsTriggerAvailableNow,[Default]), UsedName(deserializeOffset,[Default]))), NamesChange(org.apache.spark.sql.SaveMode,ModifiedNames(changes = UsedName(compareTo,[Default]), UsedName(wait,[Default]), UsedName(ErrorIfExists,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(ordinal,[Default]), UsedName(Overwrite,[Default]), UsedName(values,[Default]), UsedName(Append,[Default]), UsedName(hashCode,[Default]), UsedName(getClass,[Default]), UsedName(notifyAll,[Default]), UsedName(name,[Default]), UsedName(SaveMode,[Default]), UsedName(getDeclaringClass,[Default]), UsedName(valueOf,[Default]), UsedName(toString,[Default]), UsedName(Ignore,[Default]))), NamesChange(org.apache.spark.sql.connector.metric.CustomSumMetric,ModifiedNames(changes = UsedName(wait,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(initialValue,[Default]), UsedName(org;apache;spark;sql;connector;metric;CustomSumMetric;init;,[Default]), UsedName(CustomSumMetric,[Default]), UsedName(hashCode,[Default]), UsedName(aggregateTaskMetrics,[Default]), UsedName(getClass,[Default]), UsedName(description,[Default]), UsedName(notifyAll,[Default]), UsedName(name,[Default]), UsedName(toString,[Default]))), NamesChange(org.apache.spark.sql.connector.write.LogicalWriteInfo,ModifiedNames(changes = UsedName(schema,[Default]), UsedName(options,[Default]), UsedName(LogicalWriteInfo,[Default]), UsedName(queryId,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ContinuousPartitionReader,ModifiedNames(changes = UsedName(currentMetricsValues,[Default]), UsedName(close,[Default]), UsedName(ContinuousPartitionReader,[Default]), UsedName(getOffset,[Default]), UsedName(get,[Default]), UsedName(next,[Default]))), NamesChange(org.apache.spark.sql.catalyst.expressions.UnsafeArrayData,ModifiedNames(changes = UsedName(calculateHeaderPortionInBytes,[Default]), UsedName(toObjectArray,[Default]), UsedName(setShort,[Default]), UsedName(UnsafeArrayData,[Default]), UsedName(getBoolean,[Default]), UsedName(wait,[Default]), UsedName(numElements,[Default]), UsedName(toIntArray,[Default]), UsedName(writeExternal,[Default]), UsedName(getInterval,[Default]), UsedName(getDecimal,[Default]), UsedName(getSizeInBytes,[Default]), UsedName(createFreshArray,[Default]), UsedName(getInt,[Default]), UsedName(writeToMemory,[Default]), UsedName(write,[Default]), UsedName(setFloat,[Default]), UsedName(setBoolean,[Default]), UsedName(toByteArray,[Default]), UsedName(getByte,[Default]), UsedName(allocateArrayData,[Default]), UsedName(copy,[Default]), UsedName(setNullAt,[Default]), UsedName(notify,[Default]), UsedName(equals,[Default]), UsedName(toLongArray,[Default]), UsedName(toDoubleArray,[Default]), UsedName(getShort,[Default]), UsedName(toBooleanArray,[Default]), UsedName(setDouble,[Default]), UsedName(getLong,[Default]), UsedName(getMap,[Default]), UsedName(getUTF8String,[Default]), UsedName(setByte,[Default]), UsedName(read,[Default]), UsedName(toShortArray,[Default]), UsedName(toFloatArray,[Default]), UsedName(foreach,[Default]), UsedName(getBaseObject,[Default]), UsedName(getDouble,[Default]), UsedName(getArray,[Default]), UsedName(hashCode,[Default]), UsedName(getBinary,[Default]), UsedName(toArrayData,[Default]), UsedName(getFloat,[Default]), UsedName(toSeq,[Default]), UsedName(getClass,[Default]), UsedName(update,[Default]), UsedName($assertionsDisabled,[Default]), UsedName(setLong,[Default]), UsedName(fromPrimitiveArray,[Default]), UsedName(pointTo,[Default]), UsedName(notifyAll,[Default]), UsedName(writeTo,[Default]), UsedName(setInt,[Default]), UsedName(org;apache;spark;sql;catalyst;expressions;UnsafeArrayData;init;,[Default]), UsedName(getBaseOffset,[Default]), UsedName(get,[Default]), UsedName(getStruct,[Default]), UsedName(toString,[Default]), UsedName(shouldUseGenericArrayData,[Default]), UsedName(toArray,[Default]), UsedName(isNullAt,[Default]), UsedName(array,[Default]), UsedName(readExternal,[Default]))), NamesChange(org.apache.spark.sql.connector.read.streaming.ReadLimit,ModifiedNames(changes = UsedName(maxFiles,[Default]), UsedName(allAvailable,[Default]), UsedName(compositeLimit,[Default]), UsedName(maxRows,[Default]), UsedName(minRows,[Default]), UsedName(ReadLimit,[Default]))))[0m
[0m[[0m[0mdebug[0m] [0m[0mModified binary dependencies: Set(${CSR_CACHE}/https/repo1.maven.org/maven2/org/json4s/json4s-jackson_2.12/3.7.0-M11/json4s-jackson_2.12-3.7.0-M11.jar, ${CSR_CACHE}/https/repo1.maven.org/maven2/org/json4s/json4s-core_2.12/3.7.0-M11/json4s-core_2.12-3.7.0-M11.jar)[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial directly invalidated classes: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0mSources indirectly invalidated by:[0m
[0m[[0m[0mdebug[0m] [0m[0m	product: Set()[0m
[0m[[0m[0mdebug[0m] [0m[0m	binary dep: Set(${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/JsonUtils.scala)[0m
[0m[[0m[0mdebug[0m] [0m[0m	external source: Set(org.apache.spark.sql.kafka010.KafkaContinuousPartitionReader, org.apache.spark.sql.kafka010.KafkaWrite, org.apache.spark.sql.kafka010.KafkaStreamWriterFactory, org.apache.spark.sql.kafka010.KafkaOffsetRangeCalculator, org.apache.spark.sql.kafka010.KafkaRecordToRowConverter, org.apache.spark.sql.kafka010.KafkaBatchInputPartition, org.apache.spark.sql.kafka010.KafkaBatchPartitionReader, org.apache.spark.sql.kafka010.KafkaSourcePartitionOffset, org.apache.spark.sql.kafka010.KafkaDataWriterCommitMessage, org.apache.spark.sql.kafka010.OffsetOutOfRangeMetric, org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin, org.apache.spark.sql.kafka010.KafkaBatch, org.apache.spark.sql.kafka010.KafkaBatchReaderFactory, org.apache.spark.sql.kafka010.KafkaSourceOffset, org.apache.spark.sql.kafka010.KafkaSourceProvider, org.apache.spark.sql.kafka010.KafkaContinuousStream, org.apache.spark.sql.kafka010.KafkaBatchWrite, org.apache.spark.sql.kafka010.KafkaBatchWriterFactory, org.apache.spark.sql.kafka010.KafkaSource, org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable, org.apache.spark.sql.kafka010.KafkaMicroBatchStream, org.apache.spark.sql.kafka010.KafkaContinuousReaderFactory, org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer, org.apache.spark.sql.kafka010.KafkaRowWriter, org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan, org.apache.spark.sql.kafka010.KafkaContinuousInputPartition, org.apache.spark.sql.kafka010.KafkaStreamingWrite, org.apache.spark.sql.kafka010.KafkaSourceInitialOffsetWriter, org.apache.spark.sql.kafka010.KafkaDataWriter, org.apache.spark.sql.kafka010.DataLossMetric)[0m
[0m[[0m[0mdebug[0m] [0m[0mAll initially invalidated classes: Set(org.apache.spark.sql.kafka010.KafkaContinuousPartitionReader, org.apache.spark.sql.kafka010.KafkaWrite, org.apache.spark.sql.kafka010.KafkaStreamWriterFactory, org.apache.spark.sql.kafka010.KafkaOffsetRangeCalculator, org.apache.spark.sql.kafka010.KafkaRecordToRowConverter, org.apache.spark.sql.kafka010.KafkaBatchInputPartition, org.apache.spark.sql.kafka010.KafkaBatchPartitionReader, org.apache.spark.sql.kafka010.KafkaSourcePartitionOffset, org.apache.spark.sql.kafka010.KafkaDataWriterCommitMessage, org.apache.spark.sql.kafka010.OffsetOutOfRangeMetric, org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin, org.apache.spark.sql.kafka010.KafkaBatch, org.apache.spark.sql.kafka010.KafkaBatchReaderFactory, org.apache.spark.sql.kafka010.KafkaSourceOffset, org.apache.spark.sql.kafka010.KafkaSourceProvider, org.apache.spark.sql.kafka010.KafkaContinuousStream, org.apache.spark.sql.kafka010.KafkaBatchWrite, org.apache.spark.sql.kafka010.KafkaBatchWriterFactory, org.apache.spark.sql.kafka010.KafkaSource, org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable, org.apache.spark.sql.kafka010.KafkaMicroBatchStream, org.apache.spark.sql.kafka010.KafkaContinuousReaderFactory, org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer, org.apache.spark.sql.kafka010.KafkaRowWriter, org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan, org.apache.spark.sql.kafka010.KafkaContinuousInputPartition, org.apache.spark.sql.kafka010.KafkaStreamingWrite, org.apache.spark.sql.kafka010.KafkaSourceInitialOffsetWriter, org.apache.spark.sql.kafka010.KafkaDataWriter, org.apache.spark.sql.kafka010.DataLossMetric)[0m
[0m[[0m[0mdebug[0m] [0m[0mAll initially invalidated sources:Set(${BASE}/external/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/JsonUtils.scala)[0m
[0m[[0m[0mdebug[0m] [0m[0mInitial set of included nodes: org.apache.spark.sql.kafka010.KafkaContinuousPartitionReader, org.apache.spark.sql.kafka010.KafkaWrite, org.apache.spark.sql.kafka010.KafkaStreamWriterFactory, org.apache.spark.sql.kafka010.KafkaOffsetRangeCalculator, org.apache.spark.sql.kafka010.KafkaRecordToRowConverter, org.apache.spark.sql.kafka010.KafkaOffsetRange, org.apache.spark.sql.kafka010.KafkaBatchInputPartition, org.apache.spark.sql.kafka010.KafkaBatchPartitionReader, org.apache.spark.sql.kafka010.KafkaSourcePartitionOffset, org.apache.spark.sql.kafka010.KafkaDataWriterCommitMessage, org.apache.spark.sql.kafka010.OffsetOutOfRangeMetric, org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin, org.apache.spark.sql.kafka010.KafkaBatch, org.apache.spark.sql.kafka010.KafkaBatchReaderFactory, org.apache.spark.sql.kafka010.KafkaSourceOffset, org.apache.spark.sql.kafka010.KafkaSourceProvider, org.apache.spark.sql.kafka010.KafkaContinuousStream, org.apache.spark.sql.kafka010.KafkaBatchWrite, org.apache.spark.sql.kafka010.KafkaBatchWriterFactory, org.apache.spark.sql.kafka010.KafkaSource, org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaTable, org.apache.spark.sql.kafka010.KafkaWriteTask, org.apache.spark.sql.kafka010.KafkaMicroBatchStream, org.apache.spark.sql.kafka010.KafkaContinuousReaderFactory, org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer, org.apache.spark.sql.kafka010.KafkaRowWriter, org.apache.spark.sql.kafka010.KafkaSourceProvider.KafkaScan, org.apache.spark.sql.kafka010.KafkaContinuousInputPartition, org.apache.spark.sql.kafka010.KafkaSourceProvider.StrategyOnNoMatchStartingOffset, org.apache.spark.sql.kafka010.KafkaStreamingWrite, org.apache.spark.sql.kafka010.KafkaSourceInitialOffsetWriter, org.apache.spark.sql.kafka010.MockedSystemClock, org.apache.spark.sql.kafka010.KafkaDataWriter, org.apache.spark.sql.kafka010.DataLossMetric[0m
[0m[[0m[0mdebug[0m] [0m[0mIncluding org.apache.spark.sql.kafka010.KafkaDataWriter by org.apache.spark.sql.kafka010.KafkaRowWriter[0m
[0m[[0m[0mdebug[0m] [0m[0mRecompiling all sources: number of invalidated sources > 50.0% of all sources[0m
[0m[[0m[0mdebug[0m] [0m[0mcompilation cycle 1[0m
[0m[[0m[0minfo[0m] [0m[0mcompiling 31 Scala sources and 1 Java source to /apsarapangu/disk3/spark/spark-3.3.0/external/kafka-0-10-sql/target/scala-2.12/classes ...[0m
[0m[[0m[0mdebug[0m] [0m[0mGetting org.scala-sbt:compiler-bridge_2.12:1.6.0:compile for Scala 2.12.15[0m
[0m[[0m[0mdebug[0m] [0m[0m[zinc] Running cached compiler 45fe75ee for Scala compiler version 2.12.15[0m
[0m[[0m[0mdebug[0m] [0m[0m[zinc] The Scala compiler is invoked with:[0m
[0m[[0m[0mdebug[0m] [0m[0m	-unchecked[0m
[0m[[0m[0mdebug[0m] [0m[0m	-deprecation[0m
[0m[[0m[0mdebug[0m] [0m[0m	-feature[0m
[0m[[0m[0mdebug[0m] [0m[0m	-explaintypes[0m
[0m[[0m[0mdebug[0m] [0m[0m	-target:jvm-1.8[0m
[0m[[0m[0mdebug[0m] [0m[0m	-Xfatal-warnings[0m
[0m[[0m[0mdebug[0m] [0m[0m	-Ywarn-unused:imports[0m
[0m[[0m[0mdebug[0m] [0m[0m	-P:silencer:globalFilters=.*deprecated.*[0m
[0m[[0m[0mdebug[0m] [0m[0m	-unchecked[0m
[0m[[0m[0mdebug[0m] [0m[0m	-deprecation[0m
[0m[[0m[0mdebug[0m] [0m[0m	-feature[0m
[0m[[0m[0mdebug[0m] [0m[0m	-explaintypes[0m
[0m[[0m[0mdebug[0m] [0m[0m	-target:jvm-1.8[0m
[0m[[0m[0mdebug[0m] [0m[0m	-Xfatal-warnings[0m
[0m[[0m[0mdebug[0m] [0m[0m	-Ywarn-unused:imports[0m
[0m[[0m[0mdebug[0m] [0m[0m	-P:silencer:globalFilters=.*deprecated.*[0m
[0m[[0m[0mdebug[0m] [0m[0m	-P:genjavadoc:out=/apsarapangu/disk3/spark/spark-3.3.0/external/kafka-0-10-sql/target/java[0m
[0m[[0m[0mdebug[0m] [0m[0m	-P:genjavadoc:strictVisibility=true[0m
[0m[[0m[0mdebug[0m] [0m[0m	-Xplugin:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/typesafe/genjavadoc/genjavadoc-plugin_2.12.15/0.18/genjavadoc-plugin_2.12.15-0.18.jar[0m
[0m[[0m[0mdebug[0m] [0m[0m	-Xplugin:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/github/ghik/silencer-plugin_2.12.15/1.7.7/silencer-plugin_2.12.15-1.7.7.jar[0m
[0m[[0m[0mdebug[0m] [0m[0m	-Xfatal-warnings[0m
[0m[[0m[0mdebug[0m] [0m[0m	-deprecation[0m
[0m[[0m[0mdebug[0m] [0m[0m	-Ywarn-unused-import[0m
[0m[[0m[0mdebug[0m] [0m[0m	-P:silencer:globalFilters=.*deprecated.*[0m
[0m[[0m[0mdebug[0m] [0m[0m	-target:jvm-1.8[0m
[0m[[0m[0mdebug[0m] [0m[0m	-sourcepath[0m
[0m[[0m[0mdebug[0m] [0m[0m	/apsarapangu/disk3/spark/spark-3.3.0[0m
[0m[[0m[0mdebug[0m] [0m[0m	-bootclasspath[0m
[0m[[0m[0mdebug[0m] [0m[0m	/apsarapangu/disk3/spark/jdk1.8.0_333/jre/lib/resources.jar:/apsarapangu/disk3/spark/jdk1.8.0_333/jre/lib/rt.jar:/apsarapangu/disk3/spark/jdk1.8.0_333/jre/lib/sunrsasign.jar:/apsarapangu/disk3/spark/jdk1.8.0_333/jre/lib/jsse.jar:/apsarapangu/disk3/spark/jdk1.8.0_333/jre/lib/jce.jar:/apsarapangu/disk3/spark/jdk1.8.0_333/jre/lib/charsets.jar:/apsarapangu/disk3/spark/jdk1.8.0_333/jre/lib/jfr.jar:/apsarapangu/disk3/spark/jdk1.8.0_333/jre/classes:/home/chenxingguang.cxg/.sbt/boot/scala-2.12.15/lib/scala-library.jar[0m
[0m[[0m[0mdebug[0m] [0m[0m	-classpath[0m
[0m[[0m[0mdebug[0m] [0m[0m	/apsarapangu/disk3/spark/spark-3.3.0/external/kafka-0-10-sql/target/scala-2.12/classes:/apsarapangu/disk3/spark/spark-3.3.0/external/kafka-0-10-token-provider/target/scala-2.12/spark-token-provider-kafka-0-10_2.12-3.3.0.jar:/apsarapangu/disk3/spark/spark-3.3.0/common/tags/target/scala-2.12/spark-tags_2.12-3.3.0.jar:/apsarapangu/disk3/spark/spark-3.3.0/sql/core/target/scala-2.12/spark-sql_2.12-3.3.0.jar:/apsarapangu/disk3/spark/spark-3.3.0/core/target/scala-2.12/spark-core_2.12-3.3.0.jar:/apsarapangu/disk3/spark/spark-3.3.0/common/kvstore/target/scala-2.12/spark-kvstore_2.12-3.3.0.jar:/apsarapangu/disk3/spark/spark-3.3.0/common/network-shuffle/target/scala-2.12/spark-network-shuffle_2.12-3.3.0.jar:/apsarapangu/disk3/spark/spark-3.3.0/common/network-common/target/scala-2.12/spark-network-common_2.12-3.3.0.jar:/apsarapangu/disk3/spark/spark-3.3.0/common/unsafe/target/scala-2.12/spark-unsafe_2.12-3.3.0.jar:/apsarapangu/disk3/spark/spark-3.3.0/launcher/target/scala-2.12/spark-launcher_2.12-3.3.0.jar:/apsarapangu/disk3/spark/spark-3.3.0/common/sketch/target/scala-2.12/spark-sketch_2.12-3.3.0.jar:/apsarapangu/disk3/spark/spark-3.3.0/sql/catalyst/target/scala-2.12/spark-catalyst_2.12-3.3.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.2.0/scala-collection-compat_2.12-2.2.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/github/ghik/silencer-lib_2.12.15/1.7.7/silencer-lib_2.12.15-1.7.7.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/rocksdb/rocksdbjni/6.20.3/rocksdbjni-6.20.3.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/univocity/univocity-parsers/2.9.1/univocity-parsers-2.9.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/orc/orc-core/1.7.4/orc-core-1.7.4.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/orc/orc-mapreduce/1.7.4/orc-mapreduce-1.7.4.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/hive/hive-storage-api/2.7.2/hive-storage-api-2.7.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/parquet/parquet-column/1.12.2/parquet-column-1.12.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/parquet/parquet-hadoop/1.12.2/parquet-hadoop-1.12.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/fasterxml/jackson/core/jackson-databind/2.13.3/jackson-databind-2.13.3.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/xbean/xbean-asm9-shaded/4.20/xbean-asm9-shaded-4.20.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/slf4j/slf4j-api/1.7.35/slf4j-api-1.7.35.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/avro/avro/1.11.0/avro-1.11.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/avro/avro-mapred/1.11.0/avro-mapred-1.11.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/twitter/chill_2.12/0.10.0/chill_2.12-0.10.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/twitter/chill-java/0.10.0/chill-java-0.10.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/javax/activation/activation/1.1.1/activation-1.1.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/zookeeper/zookeeper/3.6.2/zookeeper-3.6.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-plus/9.4.46.v20220331/jetty-plus-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-security/9.4.46.v20220331/jetty-security-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-util/9.4.46.v20220331/jetty-util-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-server/9.4.46.v20220331/jetty-server-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-http/9.4.46.v20220331/jetty-http-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-continuation/9.4.46.v20220331/jetty-continuation-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-servlet/9.4.46.v20220331/jetty-servlet-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-proxy/9.4.46.v20220331/jetty-proxy-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-client/9.4.46.v20220331/jetty-client-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-servlets/9.4.46.v20220331/jetty-servlets-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/jakarta/servlet/jakarta.servlet-api/4.0.3/jakarta.servlet-api-4.0.3.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/commons-codec/commons-codec/1.15/commons-codec-1.15.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/commons/commons-lang3/3.12.0/commons-lang3-3.12.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/commons/commons-text/1.9/commons-text-1.9.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/repo1.maven.org/maven2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/commons/commons-collections4/4.4/commons-collections4-4.4.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/slf4j/jul-to-slf4j/1.7.32/jul-to-slf4j-1.7.32.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/slf4j/jcl-over-slf4j/1.7.32/jcl-over-slf4j-1.7.32.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/logging/log4j/log4j-slf4j-impl/2.17.2/log4j-slf4j-impl-2.17.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/logging/log4j/log4j-api/2.17.2/log4j-api-2.17.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/logging/log4j/log4j-core/2.17.2/log4j-core-2.17.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/logging/log4j/log4j-1.2-api/2.17.2/log4j-1.2-api-2.17.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/ning/compress-lzf/1.1/compress-lzf-1.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/github/luben/zstd-jni/1.5.2-1/zstd-jni-1.5.2-1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/roaringbitmap/RoaringBitmap/0.9.25/RoaringBitmap-0.9.25.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/scala-lang/modules/scala-xml_2.12/1.2.0/scala-xml_2.12-1.2.0.jar:/home/chenxingguang.cxg/.sbt/boot/scala-2.12.15/lib/scala-reflect.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/json4s/json4s-jackson_2.12/3.7.0-M11/json4s-jackson_2.12-3.7.0-M11.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/jersey/core/jersey-client/2.34/jersey-client-2.34.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/jersey/core/jersey-common/2.34/jersey-common-2.34.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/jersey/core/jersey-server/2.34/jersey-server-2.34.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/jersey/containers/jersey-container-servlet/2.34/jersey-container-servlet-2.34.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/jersey/containers/jersey-container-servlet-core/2.34/jersey-container-servlet-core-2.34.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/jersey/inject/jersey-hk2/2.34/jersey-hk2-2.34.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-all/4.1.74.Final/netty-all-4.1.74.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/clearspring/analytics/stream/2.9.6/stream-2.9.6.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/dropwizard/metrics/metrics-core/4.2.7/metrics-core-4.2.7.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/dropwizard/metrics/metrics-jvm/4.2.7/metrics-jvm-4.2.7.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/dropwizard/metrics/metrics-json/4.2.7/metrics-json-4.2.7.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/dropwizard/metrics/metrics-graphite/4.2.7/metrics-graphite-4.2.7.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/dropwizard/metrics/metrics-jmx/4.2.7/metrics-jmx-4.2.7.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.13.3/jackson-module-scala_2.12-2.13.3.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/ivy/ivy/2.5.0/ivy-2.5.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/oro/oro/2.0.8/oro-2.0.8.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/net/razorvine/pickle/1.2/pickle-1.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/net/sf/py4j/py4j/0.10.9.5/py4j-0.10.9.5.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/commons/commons-crypto/1.1.0/commons-crypto-1.1.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.12/1.1.2/scala-parser-combinators_2.12-1.1.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/codehaus/janino/janino/3.0.16/janino-3.0.16.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/codehaus/janino/commons-compiler/3.0.16/commons-compiler-3.0.16.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/arrow/arrow-vector/7.0.0/arrow-vector-7.0.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/arrow/arrow-memory-netty/7.0.0/arrow-memory-netty-7.0.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/orc/orc-shims/1.7.4/orc-shims-1.7.4.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/google/protobuf/protobuf-java/3.14.0/protobuf-java-3.14.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/airlift/aircompressor/0.21/aircompressor-0.21.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/jetbrains/annotations/17.0.0/annotations-17.0.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/threeten/threeten-extra/1.5.0/threeten-extra-1.5.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/parquet/parquet-common/1.12.2/parquet-common-1.12.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/parquet/parquet-encoding/1.12.2/parquet-encoding-1.12.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/parquet/parquet-format-structures/1.12.2/parquet-format-structures-1.12.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/parquet/parquet-jackson/1.12.2/parquet-jackson-1.12.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/fasterxml/jackson/core/jackson-annotations/2.13.3/jackson-annotations-2.13.3.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/fasterxml/jackson/core/jackson-core/2.13.3/jackson-core-2.13.3.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/google/crypto/tink/tink/1.6.1/tink-1.6.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/commons/commons-compress/1.21/commons-compress-1.21.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/avro/avro-ipc/1.11.0/avro-ipc-1.11.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/esotericsoftware/kryo-shaded/4.0.2/kryo-shaded-4.0.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/zookeeper/zookeeper-jute/3.6.2/zookeeper-jute-3.6.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/yetus/audience-annotations/0.12.0/audience-annotations-0.12.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-webapp/9.4.46.v20220331/jetty-webapp-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-jndi/9.4.46.v20220331/jetty-jndi-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-io/9.4.46.v20220331/jetty-io-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-util-ajax/9.4.46.v20220331/jetty-util-ajax-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/roaringbitmap/shims/0.9.25/shims-0.9.25.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/json4s/json4s-core_2.12/3.7.0-M11/json4s-core_2.12-3.7.0-M11.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/jakarta/ws/rs/jakarta.ws.rs-api/2.1.6/jakarta.ws.rs-api-2.1.6.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/hk2/external/jakarta.inject/2.6.1/jakarta.inject-2.6.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/jakarta/annotation/jakarta.annotation-api/1.3.5/jakarta.annotation-api-1.3.5.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/hk2/osgi-resource-locator/1.0.3/osgi-resource-locator-1.0.3.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/jakarta/validation/jakarta.validation-api/2.0.2/jakarta.validation-api-2.0.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/hk2/hk2-locator/2.6.1/hk2-locator-2.6.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/javassist/javassist/3.25.0-GA/javassist-3.25.0-GA.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-buffer/4.1.74.Final/netty-buffer-4.1.74.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-codec/4.1.74.Final/netty-codec-4.1.74.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-common/4.1.74.Final/netty-common-4.1.74.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-handler/4.1.74.Final/netty-handler-4.1.74.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-tcnative-classes/2.0.48.Final/netty-tcnative-classes-2.0.48.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-resolver/4.1.74.Final/netty-resolver-4.1.74.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport/4.1.74.Final/netty-transport-4.1.74.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport-classes-epoll/4.1.74.Final/netty-transport-classes-epoll-4.1.74.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport-native-unix-common/4.1.74.Final/netty-transport-native-unix-common-4.1.74.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport-classes-kqueue/4.1.74.Final/netty-transport-classes-kqueue-4.1.74.Final.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport-native-epoll/4.1.74.Final/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport-native-epoll/4.1.74.Final/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport-native-kqueue/4.1.74.Final/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport-native-kqueue/4.1.74.Final/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/arrow/arrow-format/7.0.0/arrow-format-7.0.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/arrow/arrow-memory-core/7.0.0/arrow-memory-core-7.0.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/google/flatbuffers/flatbuffers-java/1.12.0/flatbuffers-java-1.12.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/google/code/gson/gson/2.8.6/gson-2.8.6.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/tukaani/xz/1.9/xz-1.9.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/com/esotericsoftware/minlog/1.3.0/minlog-1.3.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/objenesis/objenesis/2.5.1/objenesis-2.5.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/eclipse/jetty/jetty-xml/9.4.46.v20220331/jetty-xml-9.4.46.v20220331.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/json4s/json4s-ast_2.12/3.7.0-M11/json4s-ast_2.12-3.7.0-M11.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/json4s/json4s-scalap_2.12/3.7.0-M11/json4s-scalap_2.12-3.7.0-M11.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/hk2/external/aopalliance-repackaged/2.6.1/aopalliance-repackaged-2.6.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/hk2/hk2-api/2.6.1/hk2-api-2.6.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/org/glassfish/hk2/hk2-utils/2.6.1/hk2-utils-2.6.1.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/javax/annotation/javax.annotation-api/1.3.2/javax.annotation-api-1.3.2.jar:/home/chenxingguang.cxg/.cache/coursier/v1/https/maven-central.storage-download.googleapis.com/maven2/io/netty/netty-transport-native-epoll/4.1.74.Final/netty-transport-native-epoll-4.1.74.Final.jar[0m
[0m[[0m[0mdebug[0m] [0m[0mScala compilation took 2.041342125 s[0m
[0m[[0m[0mdebug[0m] [0m[0mJava compilation took 0.888138138 s[0m
[0m[[0m[0mdebug[0m] [0m[0mJava analysis took 0.001654881 s[0m
[0m[[0m[0mdebug[0m] [0m[0mJava compilation + analysis took 0.892845529 s[0m
[0m[[0m[0mdebug[0m] [0m[0mdone compiling[0m
